{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0: DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=5000, \n",
    "    min_df=1, \n",
    "    max_features=15, \n",
    "    stopwords_option=True,\n",
    "    stop_words = 'english'\n",
    "):\n",
    "    \n",
    "    data = load_files(\n",
    "        r\"C:/Users/migue/Downloads/aclImdb_v1/aclImdb/train\",\n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"                  \n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    #np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    global X_text \n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=0\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=stop_words, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words='english', \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "\n",
    "\n",
    "def train_NN_classifier(X_train, y_train, X_test, y_test, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a neural network on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    X_valid_bow = vectorizer.transform(X_test)\n",
    "    input_dim = X_train_bow.shape[1]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),  # First hidden layer\n",
    "        Dropout(0.3),  # Dropout with 30% probability\n",
    "        Dense(32, activation='relu'),  # Second hidden layer\n",
    "        Dropout(0.2),  # Dropout with 20% probability\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate = 0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train_bow, y_train, epochs=50, batch_size=20, validation_data=(X_valid_bow, y_test), verbose=1, callbacks=[early_stopping])\n",
    "    return model\n",
    "\n",
    "def get_cached_NN(X_train, y_train, vectorizer, num_samples, max_features, stop_words, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_classifier_ns{num_samples}_mf{max_features}_sw{stop_words}_NN_classifier_seed0.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached logistic from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clNN = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clNN = train_NN_classifier(X_train, y_train, X_valid, y_valid, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clNN, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSICAL LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classical_lime(\n",
    "    text_sample, clNN, vectorizer,  \n",
    "    k_features=20, num_samples=50   \n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns the top (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names, feature_selection=\"auto\")\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts) \n",
    "        print('shaspe of box', bow.shape, ', text_sample:',text_sample, 'features: ', k_features, 'samples: ', num_samples)\n",
    "        # print(bow)\n",
    "        proba = clNN.predict(bow.toarray())\n",
    "        if proba.ndim == 1:  # If 1D, reshape to (num_samples, 1)\n",
    "            proba = proba.reshape(-1, 1)\n",
    "        #print('proba', proba, 'dimension', proba.shape, 'return', np.hstack((1 - proba, proba)))\n",
    "        return np.column_stack((1 - proba, proba))  # Return probabilities for both classes\n",
    "        \n",
    "        \n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features,\n",
    "        num_samples=num_samples \n",
    "    )\n",
    "    return explanation.as_list() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL ROUTINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment( \n",
    "    num_samples=500,\n",
    "    min_df=1,\n",
    "    max_features=20,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=300,\n",
    "    stop_words = 'english',\n",
    "):\n",
    "\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option,\n",
    "        stop_words = stop_words\n",
    "    )\n",
    "    # B) Train model\n",
    "\n",
    "    clNN = get_cached_NN(X_train, y_train, vectorizer, num_samples, max_features, stop_words, X_test, y_test)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    test_acc = accuracy_score(y_test, clNN.predict(X_test_bow.toarray()) > 0.5)\n",
    "\n",
    "    lime_times = []\n",
    "\n",
    "    instance_local_accuracies = []\n",
    "    #5,6,12,11,10, 0, 1, 2, 3, 4 \n",
    "    #sample_indices = [5,6,12,11,10, 0, 1, 2, 3, 4]\n",
    "\n",
    "    X_all = X_train + X_test  # assuming they are lists\n",
    "    y_all = np.concatenate([y_train, y_test])  # assuming y_train and y_test are numpy arrays\n",
    "\n",
    "    for idx in range(len(X_all)):\n",
    "        text_sample = X_all[idx]\n",
    "        y_true = y_all[idx]\n",
    "\n",
    "        # 1) Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(\n",
    "            text_sample, clNN, vectorizer, \n",
    "            k_features=max_features, num_samples=lime_num_samples\n",
    "        )\n",
    "\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        y_pred = clNN.predict(bow.toarray())[0].item()\n",
    "        y_pred_label = 1 if y_pred >= 0.5 else 0\n",
    "\n",
    "        instance_accuracy = int(y_pred_label == y_true)\n",
    "\n",
    "        print(\"instance_accuracy:\", instance_accuracy)\n",
    "        instance_local_accuracies.append(instance_accuracy)\n",
    "\n",
    "   \n",
    "        lime_time = time.time() - start_lime\n",
    "        lime_times.append(lime_time)\n",
    "\n",
    "        contributions_lime_abs = [(word, abs(score)) for word, score in explanation_lime]\n",
    "        \n",
    "        print(\"idx\", idx, \"text sample\", text_sample)#, \"bin_features\", bin_features)\n",
    "     \n",
    "        word_weights = defaultdict(list)\n",
    "\n",
    "        for word, weight in contributions_lime_abs:\n",
    "        \n",
    "            word_weights[word].append(weight)\n",
    "\n",
    "        global_avg_weights = {word: sum(weights) / len(weights) for word, weights in word_weights.items()}\n",
    "\n",
    "        threshold = 0.01\n",
    "        filtered_words = {word: avg for word, avg in global_avg_weights.items() if avg >= threshold}\n",
    "        rubish_words = {word: avg for word, avg in global_avg_weights.items() if avg <= threshold}\n",
    "\n",
    "        for word in filtered_words:\n",
    "            local_weights = word_weights[word]\n",
    "            print(f\"Word: {word}, local weights: {local_weights}\")\n",
    "\n",
    "\n",
    "    results = {\n",
    "        \"local_accuracy\": np.mean(instance_local_accuracies),\n",
    "        \"lime_time_avg\": round(np.mean(lime_times), 4),\n",
    "        \"global_acc\": np.mean(test_acc)\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=10, max_features=50, stopwords=True, lime_num_samples=300, stop_words=english,\n",
      "Loading cached logistic from cached_classifier_ns10_mf50_swenglish_NN_classifier_seed0.pkl\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "shaspe of box (300, 50) , text_sample: i've just had the evidence that confirmed my suspicions. a bunch of kids, 14 to 22 put on the dvd of \"titanic\" on a fantastic state of the art mega screen home entertainment type deal. only two of them had actually seen it before. but they all had seen the moment of kate, leo and celine dion so many times that most of them felt they had seen the whole movie. shortly after the epic started, they started to get restless, some of them left asking the others -- \"call us when the iceberg appears\" over an hour and a half into the movie, only the two girls who had seen the movie before, were still there. they started shouting: iceberg, iceberg. a stampede followed, they all came back to see the sinking of the titanic. they sat open mouthed, emitting ohs and far outs. so, just like i thought when the movie first burst into the scene. what is this? one and a half hours waiting for the bloody thing to sink but what about the rest of the of it. dr. zivagho, for instance, had a similar running time, but think how much takes place in that film within the same period of time. in \"titanic\" leo teaches kate how to spit. look at the faces and hands of the, supposedly, creme de la creme in the first class dining room of the ship. look at the historical details, if you can find them. the storyline is so thin that they have to introduce guns and shootings in a ship that is about to sink. the real sinking here is of film standards. all the efforts are focus on special effects and opening week ends. the film went on to become the highest grossing movie of all time so, what do i know? features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "instance_accuracy: 0\n",
      "idx 0 text sample i've just had the evidence that confirmed my suspicions. a bunch of kids, 14 to 22 put on the dvd of \"titanic\" on a fantastic state of the art mega screen home entertainment type deal. only two of them had actually seen it before. but they all had seen the moment of kate, leo and celine dion so many times that most of them felt they had seen the whole movie. shortly after the epic started, they started to get restless, some of them left asking the others -- \"call us when the iceberg appears\" over an hour and a half into the movie, only the two girls who had seen the movie before, were still there. they started shouting: iceberg, iceberg. a stampede followed, they all came back to see the sinking of the titanic. they sat open mouthed, emitting ohs and far outs. so, just like i thought when the movie first burst into the scene. what is this? one and a half hours waiting for the bloody thing to sink but what about the rest of the of it. dr. zivagho, for instance, had a similar running time, but think how much takes place in that film within the same period of time. in \"titanic\" leo teaches kate how to spit. look at the faces and hands of the, supposedly, creme de la creme in the first class dining room of the ship. look at the historical details, if you can find them. the storyline is so thin that they have to introduce guns and shootings in a ship that is about to sink. the real sinking here is of film standards. all the efforts are focus on special effects and opening week ends. the film went on to become the highest grossing movie of all time so, what do i know?\n",
      "Word: like, local weights: [0.026269161094225217]\n",
      "Word: time, local weights: [0.025709249955307514]\n",
      "Word: far, local weights: [0.02535369666330291]\n",
      "Word: appears, local weights: [0.02514433978859683]\n",
      "Word: seen, local weights: [0.02424673763815439]\n",
      "Word: effects, local weights: [0.022480999944793755]\n",
      "Word: just, local weights: [0.01852797204800271]\n",
      "Word: times, local weights: [0.016867465495590483]\n",
      "Word: art, local weights: [0.016623707897456416]\n",
      "Word: left, local weights: [0.014060986825186457]\n",
      "Word: spit, local weights: [0.010263407918165912]\n",
      "shaspe of box (300, 50) , text_sample: i had no idea that mr. izzard was so damn funny, it really boggles the mind that he is not more well known! his command over the crowd and his timing is perfect.the monologue about star wars will kill ya too! if only all the stand up performers had his wit... features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "instance_accuracy: 1\n",
      "idx 1 text sample i had no idea that mr. izzard was so damn funny, it really boggles the mind that he is not more well known! his command over the crowd and his timing is perfect.the monologue about star wars will kill ya too! if only all the stand up performers had his wit...\n",
      "Word: perfect, local weights: [0.04669382047971996]\n",
      "Word: idea, local weights: [0.032056724768915634]\n",
      "Word: really, local weights: [0.02193272163001546]\n",
      "shaspe of box (300, 50) , text_sample: words can't describe how bad this movie is. i can't explain it by writing only. you have too see it for yourself to get at grip of how horrible a movie really can be. not that i recommend you to do that. there are so many clichés, mistakes (and all other negative things you can imagine) here that will just make you cry. to start with the technical first, there are a lot of mistakes regarding the airplane. i won't list them here, but just mention the coloring of the plane. they didn't even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original boeing livery. very bad. the plot is stupid and has been done many times before, only much, much better. there are so many ridiculous moments here that i lost count of it really early. also, i was on the bad guys' side all the time in the movie, because the good guys were so stupid. \"executive decision\" should without a doubt be you're choice over this one, even the \"turbulence\"-movies are better. in fact, every other movie in the world is better than this one. features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "instance_accuracy: 0\n",
      "idx 2 text sample words can't describe how bad this movie is. i can't explain it by writing only. you have too see it for yourself to get at grip of how horrible a movie really can be. not that i recommend you to do that. there are so many clichés, mistakes (and all other negative things you can imagine) here that will just make you cry. to start with the technical first, there are a lot of mistakes regarding the airplane. i won't list them here, but just mention the coloring of the plane. they didn't even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original boeing livery. very bad. the plot is stupid and has been done many times before, only much, much better. there are so many ridiculous moments here that i lost count of it really early. also, i was on the bad guys' side all the time in the movie, because the good guys were so stupid. \"executive decision\" should without a doubt be you're choice over this one, even the \"turbulence\"-movies are better. in fact, every other movie in the world is better than this one.\n",
      "Word: time, local weights: [0.043427815575987766]\n",
      "Word: instead, local weights: [0.029380689603886862]\n",
      "Word: times, local weights: [0.024192923417490426]\n",
      "Word: good, local weights: [0.023286325107317138]\n",
      "Word: better, local weights: [0.021234945313879736]\n",
      "Word: world, local weights: [0.019937519353701593]\n",
      "Word: used, local weights: [0.018539848106593124]\n",
      "Word: bad, local weights: [0.01747258655786385]\n",
      "Word: lot, local weights: [0.01712086232279286]\n",
      "Word: make, local weights: [0.01486644713224118]\n",
      "Word: really, local weights: [0.012674384242419463]\n",
      "Word: movie, local weights: [0.01054945744639674]\n",
      "shaspe of box (300, 50) , text_sample: this movie has a special way of telling the story, at first i found it rather odd as it jumped through time and i had no idea whats happening.anyway the story line was although simple, but still very real and touching. you met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. who hasn't go through this? but we will never forget this kind of pain in our life. i would say i am rather touched as two actor has shown great performance in showing the love between the characters. i just wish that the story could be a happy ending. features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "instance_accuracy: 1\n",
      "idx 3 text sample this movie has a special way of telling the story, at first i found it rather odd as it jumped through time and i had no idea whats happening.anyway the story line was although simple, but still very real and touching. you met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. who hasn't go through this? but we will never forget this kind of pain in our life. i would say i am rather touched as two actor has shown great performance in showing the love between the characters. i just wish that the story could be a happy ending.\n",
      "Word: time, local weights: [0.0337866221380895]\n",
      "Word: idea, local weights: [0.032936902708426176]\n",
      "Word: jumped, local weights: [0.017257685421432956]\n",
      "Word: pain, local weights: [0.015821548576124698]\n",
      "Word: fell, local weights: [0.011858410869895577]\n",
      "Word: real, local weights: [0.010510646295063505]\n",
      "shaspe of box (300, 50) , text_sample: the single worst film i've ever seen in a theater. i saw this film at the austin film festival in 2004, and it blew my mind that this film was accepted to a festival. it was an interesting premise, and seemed like it could go somewhere, but just fell apart every time it tried to do anything. first of all, if you're going to do a musical, find someone with musical talent. the music consisted of cheesy piano playing that sounded like they were playing it on a stereo in the room they were filming. the lyrics were terribly written, and when they weren't obvious rhymes, they were groan-inducing rhymes that showed how far they were stretching to try to make this movie work. and you'd think you'd find people who could sing when making a musical, right? not in this case. luckily they were half talking/half singing in rhyme most of the time, but when they did sing it made me cringe. especially when they attempted to sing in harmony. and that just addresses the music. some of the acting was pretty good, but a lot of the dialog was terrible, as well as most of the scenes. they obviously didn't have enough coverage on the scenes, or they just had a bad editor, because they consistently jumped the line and used terrible choices while cutting the film. at least the director was willing to admit that no one wanted the script until they added the hook of making it a musical. i hope the investors make sure someone can write music before making the same mistake again. features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "instance_accuracy: 1\n",
      "idx 4 text sample the single worst film i've ever seen in a theater. i saw this film at the austin film festival in 2004, and it blew my mind that this film was accepted to a festival. it was an interesting premise, and seemed like it could go somewhere, but just fell apart every time it tried to do anything. first of all, if you're going to do a musical, find someone with musical talent. the music consisted of cheesy piano playing that sounded like they were playing it on a stereo in the room they were filming. the lyrics were terribly written, and when they weren't obvious rhymes, they were groan-inducing rhymes that showed how far they were stretching to try to make this movie work. and you'd think you'd find people who could sing when making a musical, right? not in this case. luckily they were half talking/half singing in rhyme most of the time, but when they did sing it made me cringe. especially when they attempted to sing in harmony. and that just addresses the music. some of the acting was pretty good, but a lot of the dialog was terrible, as well as most of the scenes. they obviously didn't have enough coverage on the scenes, or they just had a bad editor, because they consistently jumped the line and used terrible choices while cutting the film. at least the director was willing to admit that no one wanted the script until they added the hook of making it a musical. i hope the investors make sure someone can write music before making the same mistake again.\n",
      "Word: time, local weights: [0.04630157553663249]\n",
      "Word: lot, local weights: [0.031248061410826564]\n",
      "Word: movie, local weights: [0.02061748998694084]\n",
      "Word: good, local weights: [0.020537195343750474]\n",
      "Word: half, local weights: [0.0179960317111338]\n",
      "Word: like, local weights: [0.017144463905497915]\n",
      "Word: used, local weights: [0.01660994712425116]\n",
      "Word: film, local weights: [0.01571087003355132]\n",
      "Word: bad, local weights: [0.014324544716858725]\n",
      "Word: people, local weights: [0.012430065428663793]\n",
      "Word: terrible, local weights: [0.012136181763529154]\n",
      "Word: interesting, local weights: [0.011737094985146959]\n",
      "Word: just, local weights: [0.011476801392891197]\n",
      "Word: addresses, local weights: [0.011371279384889564]\n",
      "shaspe of box (300, 50) , text_sample: there are a lot of highly talented filmmakers/actors in germany now. none of them are associated with this \"movie\".why in the world do producers actually invest money in something like this this? you could have made 10 good films with the budget of this garbage! it's not entertaining to have seven grown men running around as dwarfs, pretending to be funny. what is funny though is that the film's producer (who happens to be the oldest guy of the bunch) is playing the youngest dwarf.the film is filled with moments that scream for captions saying \"you're supposed to laugh now!\". it's hard to believe that this crap's supposed to be a comedy.many people actually stood up and left the cinema 30 minutes into the movie. i should have done the same instead of wasting my time...pain! features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "instance_accuracy: 1\n",
      "idx 5 text sample there are a lot of highly talented filmmakers/actors in germany now. none of them are associated with this \"movie\".why in the world do producers actually invest money in something like this this? you could have made 10 good films with the budget of this garbage! it's not entertaining to have seven grown men running around as dwarfs, pretending to be funny. what is funny though is that the film's producer (who happens to be the oldest guy of the bunch) is playing the youngest dwarf.the film is filled with moments that scream for captions saying \"you're supposed to laugh now!\". it's hard to believe that this crap's supposed to be a comedy.many people actually stood up and left the cinema 30 minutes into the movie. i should have done the same instead of wasting my time...pain!\n",
      "Word: time, local weights: [0.03540351803326352]\n",
      "Word: lot, local weights: [0.033038096312297344]\n",
      "Word: men, local weights: [0.030500230361555587]\n",
      "Word: actually, local weights: [0.0257980391896908]\n",
      "Word: money, local weights: [0.024601466733157332]\n",
      "Word: left, local weights: [0.023312236384542124]\n",
      "Word: film, local weights: [0.020168718035328274]\n",
      "Word: like, local weights: [0.0192220628318163]\n",
      "Word: actors, local weights: [0.014847894590283895]\n",
      "Word: instead, local weights: [0.013907786228891952]\n",
      "Word: playing, local weights: [0.01380137921535313]\n",
      "Word: funny, local weights: [0.01343432276657471]\n",
      "Word: movie, local weights: [0.011797252900522671]\n",
      "shaspe of box (300, 50) , text_sample: zero day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.it is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. in terms of explaining the motives and actions of the two young suicide/murderers it is better than 'elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. flawed but honest with a terrible honesty. features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "instance_accuracy: 1\n",
      "idx 6 text sample zero day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. it captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.it is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. in terms of explaining the motives and actions of the two young suicide/murderers it is better than 'elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. flawed but honest with a terrible honesty.\n",
      "Word: time, local weights: [0.037749575093796936]\n",
      "Word: terrible, local weights: [0.028840385417954105]\n",
      "Word: far, local weights: [0.02813539366893417]\n",
      "Word: world, local weights: [0.02552036093812014]\n",
      "Word: better, local weights: [0.013349290056549366]\n",
      "Word: money, local weights: [0.011255143973959022]\n",
      "Word: did, local weights: [0.011249397201905393]\n",
      "Word: movie, local weights: [0.010490784351917031]\n",
      "shaspe of box (300, 50) , text_sample: the movie was sub-par, but this television pilot delivers a great springboard into what has become a sci-fi fans ideal program. the actors deliver and the special effects (for a television series) are spectacular. having an intelligent interesting script doesn't hurt either.stargate sg1 is currently one of my favorite programs. features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "instance_accuracy: 1\n",
      "idx 7 text sample the movie was sub-par, but this television pilot delivers a great springboard into what has become a sci-fi fans ideal program. the actors deliver and the special effects (for a television series) are spectacular. having an intelligent interesting script doesn't hurt either.stargate sg1 is currently one of my favorite programs.\n",
      "Word: actors, local weights: [0.03392843924953521]\n",
      "Word: effects, local weights: [0.020798537649391033]\n",
      "Word: special, local weights: [0.019881070352293726]\n",
      "Word: script, local weights: [0.011307619154107567]\n",
      "shaspe of box (300, 50) , text_sample: everyone plays their part pretty well in this \"little nice movie\". belushi gets the chance to live part of his life differently, but ends up realizing that what he had was going to be just as good or maybe even better. the movie shows us that we ought to take advantage of the opportunities we have, not the ones we do not or cannot have. if u can get this movie on video for around $10, it´d be an investment! features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "instance_accuracy: 1\n",
      "idx 8 text sample everyone plays their part pretty well in this \"little nice movie\". belushi gets the chance to live part of his life differently, but ends up realizing that what he had was going to be just as good or maybe even better. the movie shows us that we ought to take advantage of the opportunities we have, not the ones we do not or cannot have. if u can get this movie on video for around $10, it´d be an investment!\n",
      "Word: better, local weights: [0.033020687489379044]\n",
      "shaspe of box (300, 50) , text_sample: the plot of this terrible film is so convoluted i've put the spoiler warning up because i'm unsure if i'm giving anything away. the audience first sees some man in jack the ripper garb murder an old man in an alley a hundred years ago. then we're up to modern day and a young australian couple is looking for a house. we're given an unbelievably long tour of this house and the husband sees a figure in an old mirror. some 105 year old woman lived there. there are also large iron panels covering a wall in the den. an old fashioned straight-razor falls out when they're renovating and the husband keeps it. i guess he becomes possessed by the razor because he starts having weird dreams. oh yeah, the couple is unable to have a baby because the husband is firing blanks. some mold seems to be climbing up the wall after the couple removes the iron panels and the mold has the shape of a person. late in the story there is a plot about a large cache of money & the husband murders the body guard & a co-worker and steals the money. his wife is suddenly pregnant. what the hell is going on?? who knows?? nothing is explained. was the 105 year old woman the child of the serial killer? the baby sister? why were iron panels put on the wall? how would that keep the serial killer contained in the cellar? was he locked down there by his family & starved to death or just concealed? who is mr. hobbs and why is he so desperate to get the iron panels?? he's never seen again. why was the serial killer killing people? we only see the one old man murdered. was there a pattern or motive or something?? why does the wife suddenly become pregnant? is it the demon spawn of the serial killer? has he managed to infiltrate the husband's semen? and why, if the husband was able to subdue and murder a huge, burly security guard, is he unable to overpower his wife? and just how powerful is the voltage system in australia that it would knock him across the room simply cutting a light wire? and why does the wife stay in the house? is she now possessed by the serial killer? is the baby going to be the killer reincarnated? this movie was such a frustrating experience i wanted to call my pbs station and ask for my money back! the only enjoyable aspect of this story was seeing the husband running around in just his boxer shorts for a lot of the time, but even that couldn't redeem this muddled, incoherent mess. features:  50 samples:  300\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "instance_accuracy: 0\n",
      "idx 9 text sample the plot of this terrible film is so convoluted i've put the spoiler warning up because i'm unsure if i'm giving anything away. the audience first sees some man in jack the ripper garb murder an old man in an alley a hundred years ago. then we're up to modern day and a young australian couple is looking for a house. we're given an unbelievably long tour of this house and the husband sees a figure in an old mirror. some 105 year old woman lived there. there are also large iron panels covering a wall in the den. an old fashioned straight-razor falls out when they're renovating and the husband keeps it. i guess he becomes possessed by the razor because he starts having weird dreams. oh yeah, the couple is unable to have a baby because the husband is firing blanks. some mold seems to be climbing up the wall after the couple removes the iron panels and the mold has the shape of a person. late in the story there is a plot about a large cache of money & the husband murders the body guard & a co-worker and steals the money. his wife is suddenly pregnant. what the hell is going on?? who knows?? nothing is explained. was the 105 year old woman the child of the serial killer? the baby sister? why were iron panels put on the wall? how would that keep the serial killer contained in the cellar? was he locked down there by his family & starved to death or just concealed? who is mr. hobbs and why is he so desperate to get the iron panels?? he's never seen again. why was the serial killer killing people? we only see the one old man murdered. was there a pattern or motive or something?? why does the wife suddenly become pregnant? is it the demon spawn of the serial killer? has he managed to infiltrate the husband's semen? and why, if the husband was able to subdue and murder a huge, burly security guard, is he unable to overpower his wife? and just how powerful is the voltage system in australia that it would knock him across the room simply cutting a light wire? and why does the wife stay in the house? is she now possessed by the serial killer? is the baby going to be the killer reincarnated? this movie was such a frustrating experience i wanted to call my pbs station and ask for my money back! the only enjoyable aspect of this story was seeing the husband running around in just his boxer shorts for a lot of the time, but even that couldn't redeem this muddled, incoherent mess.\n",
      "Word: time, local weights: [0.04485541532326854]\n",
      "Word: running, local weights: [0.027477252182413422]\n",
      "Word: people, local weights: [0.025825496098685178]\n",
      "Word: money, local weights: [0.025542967264729058]\n",
      "Word: seen, local weights: [0.022389906594748762]\n",
      "Word: ve, local weights: [0.011022618946732426]\n",
      "Results => {'num_samples': 10, 'max_features': 50, 'stopwords': True, 'lime_num_samples': 300, 'local_accuracy': np.float64(0.7), 'lime_time_avg': np.float64(0.4434), 'global_acc': np.float64(0.5)}\n",
      "\n",
      "All done! Saved results to 'results_expanded_flips.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Parameter grid to systematically vary certain settings\n",
    "    param_grid = {\n",
    "        \"num_samples\": [1000],\n",
    "        \"max_features\": [500],\n",
    "        \"stopwords_option\": [True],\n",
    "        \"lime_num_samples\": [300],\n",
    "        \"stop_words\": ['english'],\n",
    "           \n",
    "    }\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    all_results = []\n",
    "\n",
    "    for combo in combos:\n",
    "        (num_samples_, max_features_, stopwords_, lime_samps_, stop_words_) = combo\n",
    "        \n",
    "        print(\"\\n==================================\")\n",
    "        print(f\"Running experiment with: \"\n",
    "              f\"num_samples={num_samples_}, \"\n",
    "              f\"max_features={max_features_}, \"\n",
    "              f\"stopwords={stopwords_}, \"\n",
    "              f\"lime_num_samples={lime_samps_}, \"\n",
    "              f\"stop_words={stop_words_},\")\n",
    "        \n",
    "        res = run_experiment(\n",
    "            num_samples=num_samples_,\n",
    "            max_features=max_features_,\n",
    "            stopwords_option=stopwords_,\n",
    "            lime_num_samples=lime_samps_,\n",
    "            stop_words=stop_words_,\n",
    "        )\n",
    "        res_row = {\n",
    "            \"num_samples\": num_samples_,\n",
    "            \"max_features\": max_features_,\n",
    "            \"stopwords\": stopwords_,\n",
    "            \"lime_num_samples\": lime_samps_,\n",
    "            \"local_accuracy\": res[\"local_accuracy\"],\n",
    "            \"lime_time_avg\": res[\"lime_time_avg\"],\n",
    "            \"global_acc\": res[\"global_acc\"]\n",
    "        }\n",
    "        print(\"Results =>\", res_row)\n",
    "        all_results.append(res_row)\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"results_expanded_flips.csv\", index=False)\n",
    "    print(\"\\nAll done! Saved results to 'results_expanded_flips.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
