{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classical LIME\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0: DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove anything between <...> tags, then lowercase the text\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=5000, \n",
    "    min_df=1, \n",
    "    max_features=15, \n",
    "    stopwords_option=True,\n",
    "    stop_words = 'english'\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \n",
    "    Now with text cleaning for HTML, lowercase, etc.\n",
    "    \"\"\"\n",
    "    data = load_files(\n",
    "        \"C:/Users/migue/Downloads/aclImdb_v1/aclImdb/train\",\n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"                  \n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "    # Clean text (HTML removal + lowercase)\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    #np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=stop_words, #'English' is a countvectorizer list made by sklearn to skip common words\n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=None, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "#def train_XGBoost_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    # Use log(len(y_train)) as n_estimators (rounded to an int)\n",
    "    clXGB = XGBClassifier(\n",
    "        #booster=\"gblinear\",\n",
    "        objective=\"binary:logistic\", \n",
    "        eval_metric=\"logloss\", \n",
    "        random_state=42, \n",
    "        n_estimators=int(round(math.log(len(y_train)))),\n",
    "        learning_rate=0.1, \n",
    "        max_depth=3\n",
    "    )\n",
    "    clXGB.fit(X_train_bow, y_train)\n",
    "    return clXGB\n",
    "\n",
    "#def get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_xgboost_ns{num_samples}_mf{max_features}_sw{stopwords_option}_xgboost_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached xgboost from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clXGB = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clXGB = train_XGBoost_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clXGB, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clXGB\n",
    "\n",
    "def train_logistic_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    return clf\n",
    "\n",
    "def get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_classifier_ns{num_samples}_mf{max_features}_sw{stop_words}_logistic_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached logistic from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clf\n",
    "\n",
    "#def train_lasso_regression(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    lasso_model = Lasso(alpha=0.5)\n",
    "    lasso_model.fit(X_train_bow, y_train)\n",
    "    return lasso_model\n",
    "    \n",
    "\n",
    "#def get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha):\n",
    "    \"\"\"\n",
    "    Checks if a Lasso model trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_lasso_ns{num_samples}_mf{max_features}_sw{stopwords_option}_seed42_alpha{alpha}.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached Lasso model from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            lasso_model = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached Lasso model found. Training a new one...\")\n",
    "        lasso_model = train_lasso_regression(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(lasso_model, f)\n",
    "        print(\"Cached Lasso model saved as\", filename)\n",
    "    return lasso_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSICAL LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#CHANGE clXGB TO clf IF WE WANT LOGISTIC INSTEAD OF XGBOOST\n",
    "def run_classical_lime(\n",
    "    text_sample, clXGB, vectorizer, \n",
    "    k_features=10, num_samples=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns the top (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names, feature_selection=\"auto\")\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts) \n",
    "        return clXGB.predict_proba(bow)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features,\n",
    "        num_samples=num_samples  # e.g. 300 or 500\n",
    "    )\n",
    "    return explanation.as_list()  # list of (word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LIME Pi (Flip Only 1->0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_classifier(features, weights, bias=0.0, threshold=0.01):\n",
    "    # Ensure inputs are 1D arrays\n",
    "    features = np.array(features).flatten()\n",
    "    weights = np.array(weights).flatten()\n",
    "    \n",
    "    # Zero out small weights\n",
    "    sparse_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
    "    score = bias + np.dot(features, sparse_weights)\n",
    "    return float(1 / (1 + np.exp(-score)))\n",
    "\n",
    "def encode_and_flip(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Encode features -> quantum circuit.\n",
    "    FLIP ONLY if bit == 1 at flip_index (1->0).\n",
    "    \"\"\"\n",
    "    num_qubits = len(features)\n",
    "    dev = qml.device(\"default.qubit\", wires=num_qubits, shots=shots)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit():\n",
    "        for i, f in enumerate(features):\n",
    "            if i == flip_index and f == 1:   #;;  This line is the original code commented out\n",
    "                # 1->0 => RY(0),\n",
    "                theta = 0\n",
    "                #My suggestion: \n",
    "                #theta = f * (np.pi / 2)            \n",
    "                #qml.PauliX(wires=i)\n",
    "            else:\n",
    "                theta = f * (np.pi / 2) \n",
    " \n",
    "            qml.RY(theta, wires=i)\n",
    "            \n",
    "        return qml.probs(wires=range(num_qubits))\n",
    "\n",
    "    return circuit()\n",
    "\n",
    "def sample_state(probabilities):\n",
    "    \"\"\"\n",
    "    Sample an integer state index from the distribution.\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    cumsum = 0.0\n",
    "    for idx, p in enumerate(probabilities):\n",
    "        cumsum += p\n",
    "        if r <= cumsum:\n",
    "            return idx\n",
    "    return len(probabilities) - 1\n",
    "\n",
    "def measure_and_map_to_classical(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Run the circuit, measure, return a binary array for the top-likelihood state.\n",
    "    \"\"\"\n",
    "    probs = encode_and_flip(features, flip_index=flip_index, shots=shots)\n",
    "    measured_state = sample_state(probs)\n",
    "    num_qubits = len(features)\n",
    "    bin_string = f\"{measured_state:0{num_qubits}b}\"\n",
    "    return [int(bit) for bit in bin_string]\n",
    "\n",
    "\n",
    "def quantum_lime_explanation(\n",
    "    features, weights, bias=0.0, shots=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Flip only features that are 1 -> 0.\n",
    "    Return array of shape (n_features,) with:\n",
    "       Delta f_k = (original_pred - new_pred).\n",
    "    \"\"\"\n",
    "    original_pred = classical_classifier(features, weights, bias=bias)\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        new_vec = measure_and_map_to_classical(features, flip_index=i, shots=None)\n",
    "        new_pred = classical_classifier(new_vec, weights, bias=bias)\n",
    "        return original_pred - new_pred\n",
    "\n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) if val == 1 # ;  This is the original code commented out \n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions\n",
    "\n",
    "#def quantum_lime_explanation(features, clf, lasso, shots=None):\n",
    "    \n",
    "    # Reshape features for prediction (ensure 2D array)\n",
    "    features_reshaped = np.array(features).reshape(1, -1)\n",
    "    original_pred = clf.predict_proba(features_reshaped)[0, 1]\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        # Create a new feature vector with feature i flipped from 1 to 0\n",
    "        new_vec = features.copy()\n",
    "        new_vec[i] = 0\n",
    "        new_vec_reshaped = np.array(new_vec).reshape(1, -1)\n",
    "        new_pred = lasso.predict(new_vec_reshaped)[0]\n",
    "        return original_pred - new_pred\n",
    " \n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) if val == 1\n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL ROUTINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment( #Did I change these numbers? check if i fcked up smth here!!!!!!!!!!\n",
    "    num_samples=10,\n",
    "    min_df=1,\n",
    "    max_features=15,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=30,\n",
    "    shots=None,\n",
    "    n_test_explanations=10,\n",
    "    stop_words = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load data with given params (includes text cleaning)\n",
    "    2) Train logistic classifier\n",
    "    3) Evaluate test accuracy\n",
    "    4) Pick n_test_explanations random samples\n",
    "    5) For each, run classical LIME vs. Q-LIME Pi\n",
    "    6) Return summary stats\n",
    "    \"\"\"\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option,\n",
    "        stop_words = stop_words\n",
    "    )\n",
    "    # B) Train model\n",
    "    clf  = get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "\n",
    "    #clXGB = get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    #test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    print(\"Global accuracy:\", test_acc) \n",
    "\n",
    "    logistic_weights = clf.coef_[0]\n",
    "    bias = clf.intercept_[0]\n",
    "\n",
    "\n",
    "    #IT ONLY GIVES 1 WEIGHT NOT 15\n",
    "    #logistic_weights = clXGB.coef_[0]\n",
    "    #bias = clXGB.intercept_[0]\n",
    "\n",
    "\n",
    "    #lasso_model = get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha=0.1)\n",
    "    \n",
    "\n",
    "    # We'll track times & top-feature overlap\n",
    "    lime_times = []\n",
    "    qlime_times = []\n",
    "    overlaps = []\n",
    "    instance_local_accuracies = []\n",
    "\n",
    "    # Random samples for explanation\n",
    "    #n_test = len(X_test)\n",
    "    sample_indices = list(range(10))\n",
    "    #random.sample(range(n_test), n_test_explanations)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "        y_true = y_test[idx]\n",
    "\n",
    "        # 1) Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(\n",
    "            text_sample, clf, vectorizer, \n",
    "            k_features=15, num_samples=lime_num_samples\n",
    "        )\n",
    "\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        y_pred = clf.predict(bow)[0]\n",
    "        instance_accuracy = int(y_pred == y_true)\n",
    "        instance_local_accuracies.append(instance_accuracy)\n",
    "\n",
    "        #explanation_lime = run_classical_lime(\n",
    "        #    text_sample, clf, vectorizer, \n",
    "        #    k_features=15, num_samples=lime_num_samples\n",
    "        #)\n",
    "        lime_time = time.time() - start_lime\n",
    "        lime_times.append(lime_time)\n",
    "\n",
    "        # parse top features\n",
    "        lime_dict = dict(explanation_lime)\n",
    "        top_words_lime = sorted(\n",
    "            lime_dict.keys(),\n",
    "            key=lambda w: abs(lime_dict[w]),\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        # 2) Q-LIME Pi\n",
    "        \n",
    "\n",
    "        start_qlime = time.time()\n",
    "        #contributions_qlime = quantum_lime_explanation(bin_features, clf, lasso_model, shots=shots)\n",
    "        contributions_qlime = quantum_lime_explanation(\n",
    "            bin_features, logistic_weights, bias=bias, shots=shots)\n",
    "\n",
    "\n",
    "        contributions_lime_abs = [(word, abs(score)) for word, score in explanation_lime] # Absolute values for comparison; This is a tuple. PROB SHOULD MAKE QLIME A TUPLE TOO!\n",
    "        \n",
    "        unsorted_contributions_qlime_abs = tuple(\n",
    "            (word, abs(score)) for word, score in zip(vectorizer.get_feature_names_out(), contributions_qlime)) # Absolute values for comparison\n",
    "        \n",
    "        contributions_qlime_sorted = tuple(\n",
    "        sorted(unsorted_contributions_qlime_abs, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        #print(\"X_test_bow\",X_test_bow)\n",
    "          \n",
    "        print(\"text sample\", text_sample, \"bin_features\", bin_features)\n",
    "        #, \"vectorizer\", vectorizer.get_feature_names_out(), \"contributions_qlime_abs\", contributions_qlime_abs, \"Contributions_Lime\", top_words_lime\n",
    "     \n",
    "        print(\"Classical LIME Explanation:\")\n",
    "        for word, weight in contributions_lime_abs:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "\n",
    "        print(\"\\nQ-LIME Pi Explanation:\")\n",
    "        for word, weight in contributions_qlime_sorted:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "        \n",
    "        #print(\"\\n weights\", clf.coef_[0])\n",
    "        qlime_time = time.time() - start_qlime\n",
    "        qlime_times.append(qlime_time)\n",
    "\n",
    "        # top 5 (by absolute value)\n",
    "        nonzero_indices = [\n",
    "            (i, abs(contributions_qlime[i])) \n",
    "            for i in range(len(contributions_qlime))\n",
    "        ]\n",
    "        top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "        top_words_qlime = [\n",
    "            vectorizer.get_feature_names_out()[i2]\n",
    "            for (i2, val) in top_indices_qlime\n",
    "        ]\n",
    "\n",
    "        # measure overlap\n",
    "        overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "        overlaps.append(len(overlap))\n",
    "\n",
    "    # Summary\n",
    "    results = {\n",
    "        \"local_accuracy\": np.mean(instance_local_accuracies),\n",
    "        \"lime_time_avg\": round(np.mean(lime_times), 4),\n",
    "        \"qlime_time_avg\": round(np.mean(qlime_times), 4),\n",
    "        \"overlap_avg\": round(np.mean(overlaps), 4),\n",
    "        \"global_acc\": np.mean(test_acc)\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=5000, max_features=12, stopwords=True, lime_num_samples=100, shots=100,stop_words=english,n_test_explanations=5\n",
      "Loading cached logistic from cached_classifier_ns5000_mf12_swTrue_logistic_classifier_seed42.pkl\n",
      "Global accuracy: 0.603\n",
      "text sample i can't say i'm all that experienced in misty mundae flicks having seen only a handful, but it's obvious that this was made on a shoestring, and while it might have been respectable that the filmmakers were able to make a tomb raider rip-off inside a garage, it isn't because it's completely obvious that this is what they were doing. the film only runs for forty five minutes, and this is definitely a good thing as there isn't nearly enough plot here to stretch it out for any longer. it has something to do with an evil nazi scientist (who looks about as evil as a porn star playing a nazi scientist ever could), a mummy, which is clearly a man wrapped up in toilet roll and misty - this film's version of tomb raider, who keeps her top on for much less time than angelina jolie did in the big budget version. i have to say that even in spite of its shortcomings, this film could have been better. it's got misty mundae for a start, and even better than that if you ask me is the fact that it also stars the even hotter darian caine. the pair gets to engage in all the lesbian sex that you would expect from a seduction cinema film and this is at the expense of the nonexistent plot, although that isn't really a bad thing. obviously, this is a rubbish film - but the fact that it's short is to its credit, and if you're after a bit of lesbian sex, you could do worse. bin_features [0 1 1 0 0 0 1 0 0 1 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: make, Importance: 0.08520821924273324\n",
      "Word: time, Importance: 0.01064997427184038\n",
      "Word: stretch, Importance: 0.004416773776366253\n",
      "Word: after, Importance: 0.004001166901344462\n",
      "Word: off, Importance: 0.0032805821875150736\n",
      "Word: was, Importance: 0.002850156783167683\n",
      "Word: angelina, Importance: 0.0025208375532570445\n",
      "Word: worse, Importance: 0.0020202166626980935\n",
      "Word: got, Importance: 0.0017822680548469098\n",
      "Word: flicks, Importance: 0.001504434579591007\n",
      "Word: better, Importance: 0.0012636094312592572\n",
      "Word: handful, Importance: 0.0007821595903744413\n",
      "Word: looks, Importance: 0.000645298535256205\n",
      "Word: also, Importance: 0.00030113857627220135\n",
      "Word: spite, Importance: 0.0002542406762466995\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: make, Importance: 0.09452518881306304\n",
      "Word: really, Importance: 0.09452518881306304\n",
      "Word: time, Importance: 0.09209804120192688\n",
      "Word: good, Importance: 0.07319696739639725\n",
      "Word: film, Importance: 0.015263462802005412\n",
      "Word: don, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: like, Importance: 0.0\n",
      "Word: movie, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: story, Importance: 0.0\n",
      "text sample eddie murphy is one of the funniest comedians ever - probably the funniest. delirious is the best stand-up comedy i've ever seen and it is a must-have for anyone who loves a good laugh!! i've watched this movie hundreds of times and every time i see it - i still have side-splitting fun. this is definitely one for your video library. i guarantee that you will have to watch it several times in order to hear all the jokes because you will be laughing so much - that you will miss half of them! delirious is hilarious!although there are a lot of funny comedians out there - after watching this stand-up comedy, most of them will seem like second-class citizens. if you have never seen it - get it, watch it - and you will love it!! it will make you holler!!! :-) bin_features [0 0 1 0 0 1 1 1 0 0 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: make, Importance: 0.08660675710811075\n",
      "Word: movie, Importance: 0.0862914679650336\n",
      "Word: time, Importance: 0.008010674948122092\n",
      "Word: good, Importance: 0.0070213754752305214\n",
      "Word: like, Importance: 0.005122197402889298\n",
      "Word: ve, Importance: 0.00355011794907586\n",
      "Word: it, Importance: 0.0029190947074978\n",
      "Word: second, Importance: 0.002387581857526028\n",
      "Word: be, Importance: 0.002171383707909039\n",
      "Word: all, Importance: 0.0020956711224063072\n",
      "Word: are, Importance: 0.0016792388904550645\n",
      "Word: if, Importance: 0.001313588165130744\n",
      "Word: watched, Importance: 0.0012969146292503998\n",
      "Word: order, Importance: 0.00014466835889206725\n",
      "Word: there, Importance: 5.4028873417358e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: movie, Importance: 0.18551800215948377\n",
      "Word: make, Importance: 0.09673848981192634\n",
      "Word: good, Importance: 0.08652232820352124\n",
      "Word: time, Importance: 0.019825055185866858\n",
      "Word: like, Importance: 0.006500547734831719\n",
      "Word: don, Importance: 0.0\n",
      "Word: film, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "Word: story, Importance: 0.0\n",
      "text sample if i could say it was better than gymkata, i at least felt my money was not totally wasted.then i saw steven segal's on deadly ground.this movie should see a resurrection though on mst 3k. if santa claus conquers the martians could make tom servo's head explode, one wonders what mayhem this movie could cause.there is a very good reason why kurt thomas never had a movie career.the writers of this dreck should be forced to wear placards every day of their lives that say \"bitch slap me! i was a writer on gymkata.\" bin_features [0 0 1 0 0 0 1 1 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: movie, Importance: 0.08728616725308447\n",
      "Word: make, Importance: 0.08599971138507208\n",
      "Word: good, Importance: 0.008781904332452484\n",
      "Word: steven, Importance: 0.004407436009098026\n",
      "Word: i, Importance: 0.004079849708132378\n",
      "Word: saw, Importance: 0.0029437282031434327\n",
      "Word: on, Importance: 0.0027554809414884747\n",
      "Word: this, Importance: 0.0024606884655693066\n",
      "Word: wasted, Importance: 0.002201907471931456\n",
      "Word: 3k, Importance: 0.0018838927236931129\n",
      "Word: felt, Importance: 0.0012983855715867107\n",
      "Word: then, Importance: 7.439136611048954e-05\n",
      "Word: me, Importance: 3.991961115592595e-05\n",
      "Word: reason, Importance: 3.855265130376646e-05\n",
      "Word: see, Importance: 1.148258079041957e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: make, Importance: 0.18523811465271028\n",
      "Word: movie, Importance: 0.18523811465271028\n",
      "Word: good, Importance: 0.08554746711445466\n",
      "Word: don, Importance: 0.0\n",
      "Word: film, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: like, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "Word: story, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "text sample what a load of rubbish.. i can't even begin to describe how awful this film was. the rating it has here is really hard to believe.avoid... particularly if you enjoyed the first ginger snaps. the first one was well written, well directed, well executed.. a brilliant film with a fantastic aesthetic and atmosphere. the second one was 'alrite'- decent as a self-standing film, but clearly not up to the level of the first... the third is an insult to the series, period. i rate the films: 10, 6, 1. it's that bad.oh, and yes it really is set in the past, the sisters are still called ginger and b fitzgerald... all muddled in with some half-assed native american mythology. the sisters don't have any real story, or progression, or even a clear relationship... they're just trying to survive and be 'together forever'. that's about as deep as it gets.staggered that the girls agreed to be in this pile-of-shite, after reading the script.oh and another thing, staging of action was terrible- people appearing from nowhere regularly, like the girls turn around and there's an elaborate candle-lit setup with a mystic native american woman just sitting there, about to go into a speech. sets were terrible, couldn't get away from the fact that it was all obviously based in a set, which really didn't help. also, there was consistently snow outside the camp, but not a trace inside (..on the set).arrghh,,, so bad! i really was hoping it would be at least as good as the second one. bin_features [1 1 1 0 1 1 0 0 1 1 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: don, Importance: 0.09987075081939861\n",
      "Word: just, Importance: 0.07478853208146793\n",
      "Word: story, Importance: 0.06736225738367663\n",
      "Word: s, Importance: 0.006032778913709341\n",
      "Word: couldn, Importance: 0.0056933800738608745\n",
      "Word: snow, Importance: 0.005622827319129452\n",
      "Word: 6, Importance: 0.005434164502698417\n",
      "Word: nowhere, Importance: 0.004377271314406945\n",
      "Word: half, Importance: 0.003055323752705823\n",
      "Word: aesthetic, Importance: 0.0026763972965955303\n",
      "Word: b, Importance: 0.002370461680899337\n",
      "Word: away, Importance: 0.0019330683850270385\n",
      "Word: a, Importance: 0.0017942368617177447\n",
      "Word: can, Importance: 0.001784241496571154\n",
      "Word: deep, Importance: 0.0009327559331096947\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: really, Importance: 0.21043804779514103\n",
      "Word: good, Importance: 0.20379878364682064\n",
      "Word: people, Importance: 0.1357664962122131\n",
      "Word: story, Importance: 0.13010212088892975\n",
      "Word: don, Importance: 0.10647851162454608\n",
      "Word: just, Importance: 0.10314799047933959\n",
      "Word: film, Importance: 0.029195914950862323\n",
      "Word: like, Importance: 0.0005485556503329048\n",
      "Word: great, Importance: 0.0\n",
      "Word: make, Importance: 0.0\n",
      "Word: movie, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "text sample ummm, please forgive me, but weren't more than half the characters missing? in the original novel, valjean is a man imprisoned for 19 years for stealing a loaf of bread and then attempting several times to escape. he breaks parole and is pursued relentlessly by the police inspector javert. along the way there are many characters that weren't in this version. some worth mentioning would be fantine, cosette, m & mme. thenardier, eponine, marius, gavroche, and enjolras. the only character with the same name is javert. i was confused and frustrated throughout the whole movie, trying to see how it was in any way connected to victor hugo's epic novel. bin_features [0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: movie, Importance: 0.08636691580873182\n",
      "Word: weren, Importance: 0.004384332643579894\n",
      "Word: he, Importance: 0.003286741812818383\n",
      "Word: confused, Importance: 0.0032306654725762305\n",
      "Word: thenardier, Importance: 0.0031297918428726122\n",
      "Word: times, Importance: 0.0029981162897260057\n",
      "Word: whole, Importance: 0.0027139114990466485\n",
      "Word: several, Importance: 0.0021630759089551117\n",
      "Word: please, Importance: 0.0018716695010687638\n",
      "Word: see, Importance: 0.0016620551128987947\n",
      "Word: loaf, Importance: 0.0014047015273118571\n",
      "Word: half, Importance: 0.0012853926207118571\n",
      "Word: for, Importance: 0.0010335078828633201\n",
      "Word: be, Importance: 0.0005058361374512402\n",
      "Word: ummm, Importance: 0.00047962398835289165\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: movie, Importance: 0.09899508490426706\n",
      "Word: don, Importance: 0.0\n",
      "Word: film, Importance: 0.0\n",
      "Word: good, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: like, Importance: 0.0\n",
      "Word: make, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "Word: story, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "text sample in iran, women are not permitted to attend men's sporting events, apparently to \"protect\" them from all the cursing and foul language they might hear emanating from the male fans (so since men can't restrain or behave themselves, women are forced to suffer. go figure.). \"offside\" tells the tale of a half dozen or so young women who, dressed like men, attempt to sneak into the high-stakes match between iran and bahrain that, in 2005, qualified iran to go to the world cup (the movie was actually filmed in large part during that game).\"offside\" is a slice-of-life comedy that will remind you of all those great humanistic films (\"the shop on main street,\" \"loves of a blonde,\" \"closely watched trains\" etc.) that flowed out of communist czechoslovakia as part of the \"prague miracle\" in the mid 1960's. as with many of those works, \"offside\" is more concerned with observing life than with devising any kind of elaborately contrived fictional narrative. indeed, it is the simplicity of the setup and the naturalism of the style that make the movie so effective.once their ruse is discovered, the girls are corralled into a small pen right outside the stadium where they can hear the raucous cheering emanating from the game inside. stuck where they are, all they can do is plead with the security guards to let them go in, guards who are basically bumbling, good-natured lads who are compelled to do their duty as a part of their compulsory military service. even most of the men going into the stadium don't seem particularly perturbed at the thought of these women being allowed in. still the prohibition persists. yet, how can one not be impressed by the very real courage and spunk displayed by these women as they go up against a system that continues to enforce such a ridiculously regressive and archaic restriction? and, yet, the purpose of these women is not to rally behind a cause or to make a \"point.\" they are simply obsessed fans with a burning desire to watch a soccer game and, like all the men in the country, cheer on their team.it's hard to tell just how much of the dialogue is scripted and how much of it is extemporaneous, but, in either case, the actors, with their marvelously expressive faces, do a magnificent job making each moment seem utterly real and convincing. mohammad kheir-abadi and shayesteh irani are notable standouts in a uniformly excellent cast. the structure of the film is also very loose and freeform, as writer/director jafar panahi and co-writer shadmehr rastin focus for a few brief moments on one or two of the characters, then move smoothly and effortlessly onto others. with this documentary-type approach, we come to feel as if we are witnessing an actual event unfolding in \"real time.\" very often, it's quite easy for us to forget we're actually watching a movie.it was a very smart move on the part of the filmmakers to include so much good-natured humor in the film (it's what the czech filmmakers did as well), the better to point up the utter absurdity of the situation and broaden the appeal of the film for audiences both domestic and foreign. \"offside\" is obviously a cry for justice, but it is one that is made all the more effective by its refusal to make of its story a heavy-breathing tragedy. instead, it realizes that nothing breaks down social barriers quite as efficiently as humor and an appeal to the audience's common humanity. and isn't that what true art is supposed to be all about? in its own quiet, understated way, \"offside\" is one of the great, under-appreciated gems of 2007. bin_features [1 1 1 1 1 1 1 1 0 0 1 1]\n",
      "Classical LIME Explanation:\n",
      "Word: great, Importance: 0.22692133481133125\n",
      "Word: don, Importance: 0.08899096004941358\n",
      "Word: just, Importance: 0.08083239075607151\n",
      "Word: story, Importance: 0.07804208288402176\n",
      "Word: make, Importance: 0.07596878670310774\n",
      "Word: movie, Importance: 0.07376946111217277\n",
      "Word: way, Importance: 0.010794545914952033\n",
      "Word: include, Importance: 0.010782109850759794\n",
      "Word: notable, Importance: 0.008932978034362203\n",
      "Word: efficiently, Importance: 0.008522456056681199\n",
      "Word: forget, Importance: 0.007269883400056381\n",
      "Word: type, Importance: 0.007091442608135415\n",
      "Word: stakes, Importance: 0.006566811253076601\n",
      "Word: inside, Importance: 0.0059573506392626985\n",
      "Word: you, Importance: 0.0025067055806755494\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: story, Importance: 0.24149506586827413\n",
      "Word: like, Importance: 0.20040085294259724\n",
      "Word: make, Importance: 0.2000663754240386\n",
      "Word: film, Importance: 0.1829885673116094\n",
      "Word: just, Importance: 0.17656125010632595\n",
      "Word: time, Importance: 0.17028750205651355\n",
      "Word: movie, Importance: 0.09450991730126057\n",
      "Word: great, Importance: 0.06840611175692163\n",
      "Word: don, Importance: 0.05860283197104493\n",
      "Word: good, Importance: 0.01308705657138387\n",
      "Word: people, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "text sample joseph l. mankiewicz's sleuth didn't need a remake. it's a thoroughly well made film that stands up well to this day. however, given that the modern day remake machine is currently in full swing; i really can't say i'm surprised to see the film updated for modern audience. the plot remains identical to the original film and at its core we have the story of a young man, milo tindle, who goes off to see an older man, andrew wyke, to discuss a divorce as the younger man is having an affair with the older man's wife. from there, a game of cat and mouse ensues. its clear right from the outset that director kenneth branagh wanted to add a different touch to this film and he does so by way of the central location, which has been changed from the charming games-ridden country house of the original to a technical marvel kitted out with layers of security equipment. i'm glad that the director chose to make this change as nobody wants to see a remake that directly copies of the original; plus there's the fact that the location is well used and always nice to look at. unfortunately, however, the positive elements of sleuth 2007 end there.the original film was over two hours long, while this remake is only just a shade over eighty minutes. naturally, therefore, that means that this version has less about it; and unfortunately it's the characters that suffer. the plot is also rushed and we get into the first twist in the tale far too quickly and before we are given any chance to actually understand why and how these events can be taking place. the film does not build the characters, or the rapport between them, enough to make sure that their relationship makes sense. one major thing that has been changed about the older character is his obsession; in the original he was obsessed with games which turned out to be very important once the twists come into play. here he has some kind of security fetish that doesn't really mean anything by the end. kenneth branagh's handling of the film allows for a classy score but the class ends there. the original thrived on it, but this film is happy merely to soil itself with expletives on numerous, and mostly unwarranted, occasions; which cheapen the whole thing. the final twist in the tale is completely different to how it was in the original and ensures that the film boils down to a really hideous conclusion. after spending two hours with the original i understood, respected and liked both characters presented in the film - after eighty minutes of this, i hated them both. i do have some respect for branagh for not merely rolling out a carbon copy of the original film; but this is not a good adaptation of the great anthony shaffer play. bin_features [0 1 1 1 1 0 1 0 0 1 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: great, Importance: 0.21464087586163844\n",
      "Word: make, Importance: 0.06799397635222106\n",
      "Word: just, Importance: 0.06530890911851191\n",
      "Word: story, Importance: 0.057474119027538774\n",
      "Word: tale, Importance: 0.008966164570315996\n",
      "Word: swing, Importance: 0.008867075093237196\n",
      "Word: liked, Importance: 0.008469019063208216\n",
      "Word: kitted, Importance: 0.008391604148169791\n",
      "Word: important, Importance: 0.005837221478784453\n",
      "Word: he, Importance: 0.005353244321770114\n",
      "Word: change, Importance: 0.005035085942716002\n",
      "Word: taking, Importance: 0.004838725921099002\n",
      "Word: was, Importance: 0.004584614299729143\n",
      "Word: numerous, Importance: 0.0021637205020243505\n",
      "Word: are, Importance: 0.0014014348126700172\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.26624828312754106\n",
      "Word: film, Importance: 0.15742979712421656\n",
      "Word: really, Importance: 0.12913701946309397\n",
      "Word: story, Importance: 0.0757772727476449\n",
      "Word: good, Importance: 0.05902844770894966\n",
      "Word: make, Importance: 0.0030692018104058194\n",
      "Word: just, Importance: 0.0022549141437337994\n",
      "Word: don, Importance: 0.0\n",
      "Word: like, Importance: 0.0\n",
      "Word: movie, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "text sample \"bruce almighty\" looks and sounds incredibly stupid, especially from the trailers. nevertheless, i found in it a deeper message that actually made me like this film more. bruce (jim carrey) is angry at god and is given divine powers by him to be god for a week to see if he can do a better job. morgan freeman plays a man symbolized here as god, and though it isn't his usual type of film or one of his best roles, he does excellent with what he is given to work with. although crude at times, the film does have quite a few laughs, from bruce parting his soup in half like the red sea and the customers' reactions to him, as well as freeman's seemingly laid-back and wisecracking image of god. it is overly exaggerated at times, and there is some crude humor, but overall it manages to be somewhat funny. there is a decent supporting cast, such as jennifer aniston, lisa ann walter, and steve carrell, which always helps. the end of the film proves to be very romantic and tear-jerking, and the message is clear, that we should do what god has called us to do and \"be the miracle.\" the film is far from perfect, but still enjoyable, and far better than i and many people probably would have expected, especially if we see the deeper message of the film.*** out of **** bin_features [0 1 0 0 0 1 0 0 1 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: film, Importance: 0.007778651585076105\n",
      "Word: like, Importance: 0.006163806811508973\n",
      "Word: people, Importance: 0.002977862691399507\n",
      "Word: what, Importance: 0.0004992758645989728\n",
      "Word: called, Importance: 0.0004111135990580872\n",
      "Word: it, Importance: 0.0003515013585385238\n",
      "Word: have, Importance: 0.0003438680760315799\n",
      "Word: expected, Importance: 0.00029447856991462356\n",
      "Word: parting, Importance: 0.0002696389581142073\n",
      "Word: s, Importance: 0.00019957407696462827\n",
      "Word: somewhat, Importance: 0.00014403833354859882\n",
      "Word: symbolized, Importance: 0.00014162207509725472\n",
      "Word: actually, Importance: 0.00013534720722675612\n",
      "Word: helps, Importance: 0.00011084059336175913\n",
      "Word: made, Importance: 1.6047985528327167e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: film, Importance: 0.008799072251549078\n",
      "Word: people, Importance: 0.0037642847461778484\n",
      "Word: like, Importance: 0.001865592133241889\n",
      "Word: don, Importance: 0.0\n",
      "Word: good, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: make, Importance: 0.0\n",
      "Word: movie, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "Word: story, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "text sample excellent writing and wild cast. the tech is poor but it's obviously very low budget. looks like they didn't cut the negative but had to release on a video output. in any case one of the most inventive comedies i've seen lately. the screenwriter in particular is fine. bin_features [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: like, Importance: 0.005860289278414596\n",
      "Word: didn, Importance: 0.0005181952907127536\n",
      "Word: output, Importance: 0.0003475563132735894\n",
      "Word: the, Importance: 0.00026974647806884803\n",
      "Word: writing, Importance: 0.00026425423583682344\n",
      "Word: but, Importance: 0.0002589342177285496\n",
      "Word: low, Importance: 0.0002585465858973539\n",
      "Word: most, Importance: 0.0002287469915866538\n",
      "Word: is, Importance: 0.00022163526415179305\n",
      "Word: had, Importance: 0.0002099977704579121\n",
      "Word: lately, Importance: 0.00018570107294526897\n",
      "Word: fine, Importance: 0.00017814514284413726\n",
      "Word: they, Importance: 0.00017208006094392067\n",
      "Word: looks, Importance: 0.00015847683329656435\n",
      "Word: it, Importance: 4.9195430928587084e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: like, Importance: 0.006924674542082343\n",
      "Word: don, Importance: 0.0\n",
      "Word: film, Importance: 0.0\n",
      "Word: good, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: make, Importance: 0.0\n",
      "Word: movie, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "Word: story, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "text sample louis sachar's compelling children's classic is about as disney as freddy krueger. it's got murder, racism, facial disfigurement and killer lizards.tightly plotted, it's a multi-layered, interlinking story that spans history to reveal stanley's own heritage and the secret behind the holes. it races from latvia's lush greenness to the pock-marked camp green lake (hint: there's no lake and no green).disney's first success is re-creating the novel's environments so convincingly - the set design is superb and without gloss. the other plus is in the casting. rising star shia labeouf (charlie's angels 2, project greenlight) might not be the fat boy of the book, but his attitude is right and he's far from the usual clean-cut hero. the rest of the cast is filled out equally well, from patricia arquette as the frontier school marm-turned-bank robber to henry winkler as stanley's dad. the downside is the pop soundtrack - pure marketing department - and having the sentiment turned up to full volume at the end. bin_features [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: story, Importance: 0.0683606474523999\n",
      "Word: of, Importance: 0.003728284839063914\n",
      "Word: there, Importance: 0.0032955927231456674\n",
      "Word: s, Importance: 0.0032703077064753817\n",
      "Word: other, Importance: 0.0027191040795578295\n",
      "Word: greenlight, Importance: 0.002520542983824963\n",
      "Word: louis, Importance: 0.002488605463359449\n",
      "Word: facial, Importance: 0.002230865728814962\n",
      "Word: to, Importance: 0.001976171350627421\n",
      "Word: robber, Importance: 0.0015764467351883247\n",
      "Word: set, Importance: 0.0014073669582877644\n",
      "Word: far, Importance: 0.001189298579618754\n",
      "Word: usual, Importance: 0.0009541866496289398\n",
      "Word: clean, Importance: 0.0002961547498399834\n",
      "Word: dad, Importance: 0.000156998637866041\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: story, Importance: 0.08176245875686683\n",
      "Word: don, Importance: 0.0\n",
      "Word: film, Importance: 0.0\n",
      "Word: good, Importance: 0.0\n",
      "Word: great, Importance: 0.0\n",
      "Word: just, Importance: 0.0\n",
      "Word: like, Importance: 0.0\n",
      "Word: make, Importance: 0.0\n",
      "Word: movie, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "Word: really, Importance: 0.0\n",
      "Word: time, Importance: 0.0\n",
      "Results => {'num_samples': 5000, 'max_features': 12, 'stopwords': True, 'lime_num_samples': 100, 'shots': 100, 'local_accuracy': np.float64(0.8), 'lime_time_avg': np.float64(0.0471), 'qlime_time_avg': np.float64(0.0455), 'overlap_avg': np.float64(2.3), 'global_acc': np.float64(0.603)}\n",
      "\n",
      "All done! Saved results to 'results_expanded_flips.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # Parameter grid to systematically vary certain settings\n",
    "    param_grid = {\n",
    "        \"num_samples\": [5000],\n",
    "        \"max_features\": [12],\n",
    "        \"stopwords_option\": [True],\n",
    "        \"lime_num_samples\": [100],\n",
    "        # Shots: None => analytic mode, 100 => finite sampling\n",
    "        \"shots\": [100],\n",
    "        \"stop_words\": ['english'],\n",
    "        \"n_test_explanations\": [5]\n",
    "    }\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    all_results = []\n",
    "\n",
    "    for combo in combos:\n",
    "        (num_samples_, max_features_, stopwords_, lime_samps_, shots_, stop_words_, n_test_explanations_) = combo\n",
    "        \n",
    "        print(\"\\n==================================\")\n",
    "        print(f\"Running experiment with: \"\n",
    "              f\"num_samples={num_samples_}, \"\n",
    "              f\"max_features={max_features_}, \"\n",
    "              f\"stopwords={stopwords_}, \"\n",
    "              f\"lime_num_samples={lime_samps_}, \"\n",
    "              f\"shots={shots_},\"\n",
    "              f\"stop_words={stop_words_},\"\n",
    "              f\"n_test_explanations={n_test_explanations_}\")\n",
    "        \n",
    "        res = run_experiment(\n",
    "            num_samples=num_samples_,\n",
    "            max_features=max_features_,\n",
    "            stopwords_option=stopwords_,\n",
    "            lime_num_samples=lime_samps_,\n",
    "            shots=shots_,\n",
    "            stop_words=stop_words_,\n",
    "            n_test_explanations=n_test_explanations_,\n",
    "            \n",
    "            \n",
    "        )\n",
    "        res_row = {\n",
    "            \"num_samples\": num_samples_,\n",
    "            \"max_features\": max_features_,\n",
    "            \"stopwords\": stopwords_,\n",
    "            \"lime_num_samples\": lime_samps_,\n",
    "            \"shots\": shots_,\n",
    "            \"local_accuracy\": res[\"local_accuracy\"],\n",
    "            \"lime_time_avg\": res[\"lime_time_avg\"],\n",
    "            \"qlime_time_avg\": res[\"qlime_time_avg\"],\n",
    "            \"overlap_avg\": res[\"overlap_avg\"],\n",
    "            # \"n_test_explanations\": n_test_explanations_,\n",
    "            # \"stop_words\": stop_words_,\n",
    "            \"global_acc\": res[\"global_acc\"],\n",
    "\n",
    "        }\n",
    "        print(\"Results =>\", res_row)\n",
    "        all_results.append(res_row)\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"results_expanded_flips.csv\", index=False)\n",
    "    print(\"\\nAll done! Saved results to 'results_expanded_flips.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove anything between <...> tags, then lowercase the text\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=5000, \n",
    "    min_df=1, \n",
    "    max_features=15, \n",
    "    stopwords_option=True,\n",
    "    stop_words = 'english'\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \n",
    "    Now with text cleaning for HTML, lowercase, etc.\n",
    "    \"\"\"\n",
    "    data = load_files(\n",
    "        \"C:/Users/migue/Downloads/aclImdb_v1/aclImdb/train\",\n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"                  \n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "    # Clean text (HTML removal + lowercase)\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    #np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=stop_words, #'English' is a countvectorizer list made by sklearn to skip common words\n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=None, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text sample {0} i can't say i'm all that experienced in misty mundae flicks having seen only a handful, but it's obvious that this was made on a shoestring, and while it might have been respectable that the filmmakers were able to make a tomb raider rip-off inside a garage, it isn't because it's completely obvious that this is what they were doing. the film only runs for forty five minutes, and this is definitely a good thing as there isn't nearly enough plot here to stretch it out for any longer. it has something to do with an evil nazi scientist (who looks about as evil as a porn star playing a nazi scientist ever could), a mummy, which is clearly a man wrapped up in toilet roll and misty - this film's version of tomb raider, who keeps her top on for much less time than angelina jolie did in the big budget version. i have to say that even in spite of its shortcomings, this film could have been better. it's got misty mundae for a start, and even better than that if you ask me is the fact that it also stars the even hotter darian caine. the pair gets to engage in all the lesbian sex that you would expect from a seduction cinema film and this is at the expense of the nonexistent plot, although that isn't really a bad thing. obviously, this is a rubbish film - but the fact that it's short is to its credit, and if you're after a bit of lesbian sex, you could do worse. bin_features [1 0 1 1 0 0 0 1 0 0 1 0 1 0 0]\n",
      "text sample {1} eddie murphy is one of the funniest comedians ever - probably the funniest. delirious is the best stand-up comedy i've ever seen and it is a must-have for anyone who loves a good laugh!! i've watched this movie hundreds of times and every time i see it - i still have side-splitting fun. this is definitely one for your video library. i guarantee that you will have to watch it several times in order to hear all the jokes because you will be laughing so much - that you will miss half of them! delirious is hilarious!although there are a lot of funny comedians out there - after watching this stand-up comedy, most of them will seem like second-class citizens. if you have never seen it - get it, watch it - and you will love it!! it will make you holler!!! :-) bin_features [0 0 0 1 0 0 1 1 1 0 0 0 1 1 0]\n",
      "text sample {2} if i could say it was better than gymkata, i at least felt my money was not totally wasted.then i saw steven segal's on deadly ground.this movie should see a resurrection though on mst 3k. if santa claus conquers the martians could make tom servo's head explode, one wonders what mayhem this movie could cause.there is a very good reason why kurt thomas never had a movie career.the writers of this dreck should be forced to wear placards every day of their lives that say \"bitch slap me! i was a writer on gymkata.\" bin_features [0 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
      "text sample {3} what a load of rubbish.. i can't even begin to describe how awful this film was. the rating it has here is really hard to believe.avoid... particularly if you enjoyed the first ginger snaps. the first one was well written, well directed, well executed.. a brilliant film with a fantastic aesthetic and atmosphere. the second one was 'alrite'- decent as a self-standing film, but clearly not up to the level of the first... the third is an insult to the series, period. i rate the films: 10, 6, 1. it's that bad.oh, and yes it really is set in the past, the sisters are still called ginger and b fitzgerald... all muddled in with some half-assed native american mythology. the sisters don't have any real story, or progression, or even a clear relationship... they're just trying to survive and be 'together forever'. that's about as deep as it gets.staggered that the girls agreed to be in this pile-of-shite, after reading the script.oh and another thing, staging of action was terrible- people appearing from nowhere regularly, like the girls turn around and there's an elaborate candle-lit setup with a mystic native american woman just sitting there, about to go into a speech. sets were terrible, couldn't get away from the fact that it was all obviously based in a set, which really didn't help. also, there was consistently snow outside the camp, but not a trace inside (..on the set).arrghh,,, so bad! i really was hoping it would be at least as good as the second one. bin_features [1 1 1 1 0 1 1 0 0 1 1 1 0 0 0]\n",
      "text sample {4} ummm, please forgive me, but weren't more than half the characters missing? in the original novel, valjean is a man imprisoned for 19 years for stealing a loaf of bread and then attempting several times to escape. he breaks parole and is pursued relentlessly by the police inspector javert. along the way there are many characters that weren't in this version. some worth mentioning would be fantine, cosette, m & mme. thenardier, eponine, marius, gavroche, and enjolras. the only character with the same name is javert. i was confused and frustrated throughout the whole movie, trying to see how it was in any way connected to victor hugo's epic novel. bin_features [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
      "text sample {5} in iran, women are not permitted to attend men's sporting events, apparently to \"protect\" them from all the cursing and foul language they might hear emanating from the male fans (so since men can't restrain or behave themselves, women are forced to suffer. go figure.). \"offside\" tells the tale of a half dozen or so young women who, dressed like men, attempt to sneak into the high-stakes match between iran and bahrain that, in 2005, qualified iran to go to the world cup (the movie was actually filmed in large part during that game).\"offside\" is a slice-of-life comedy that will remind you of all those great humanistic films (\"the shop on main street,\" \"loves of a blonde,\" \"closely watched trains\" etc.) that flowed out of communist czechoslovakia as part of the \"prague miracle\" in the mid 1960's. as with many of those works, \"offside\" is more concerned with observing life than with devising any kind of elaborately contrived fictional narrative. indeed, it is the simplicity of the setup and the naturalism of the style that make the movie so effective.once their ruse is discovered, the girls are corralled into a small pen right outside the stadium where they can hear the raucous cheering emanating from the game inside. stuck where they are, all they can do is plead with the security guards to let them go in, guards who are basically bumbling, good-natured lads who are compelled to do their duty as a part of their compulsory military service. even most of the men going into the stadium don't seem particularly perturbed at the thought of these women being allowed in. still the prohibition persists. yet, how can one not be impressed by the very real courage and spunk displayed by these women as they go up against a system that continues to enforce such a ridiculously regressive and archaic restriction? and, yet, the purpose of these women is not to rally behind a cause or to make a \"point.\" they are simply obsessed fans with a burning desire to watch a soccer game and, like all the men in the country, cheer on their team.it's hard to tell just how much of the dialogue is scripted and how much of it is extemporaneous, but, in either case, the actors, with their marvelously expressive faces, do a magnificent job making each moment seem utterly real and convincing. mohammad kheir-abadi and shayesteh irani are notable standouts in a uniformly excellent cast. the structure of the film is also very loose and freeform, as writer/director jafar panahi and co-writer shadmehr rastin focus for a few brief moments on one or two of the characters, then move smoothly and effortlessly onto others. with this documentary-type approach, we come to feel as if we are witnessing an actual event unfolding in \"real time.\" very often, it's quite easy for us to forget we're actually watching a movie.it was a very smart move on the part of the filmmakers to include so much good-natured humor in the film (it's what the czech filmmakers did as well), the better to point up the utter absurdity of the situation and broaden the appeal of the film for audiences both domestic and foreign. \"offside\" is obviously a cry for justice, but it is one that is made all the more effective by its refusal to make of its story a heavy-breathing tragedy. instead, it realizes that nothing breaks down social barriers quite as efficiently as humor and an appeal to the audience's common humanity. and isn't that what true art is supposed to be all about? in its own quiet, understated way, \"offside\" is one of the great, under-appreciated gems of 2007. bin_features [0 1 1 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "text sample {6} joseph l. mankiewicz's sleuth didn't need a remake. it's a thoroughly well made film that stands up well to this day. however, given that the modern day remake machine is currently in full swing; i really can't say i'm surprised to see the film updated for modern audience. the plot remains identical to the original film and at its core we have the story of a young man, milo tindle, who goes off to see an older man, andrew wyke, to discuss a divorce as the younger man is having an affair with the older man's wife. from there, a game of cat and mouse ensues. its clear right from the outset that director kenneth branagh wanted to add a different touch to this film and he does so by way of the central location, which has been changed from the charming games-ridden country house of the original to a technical marvel kitted out with layers of security equipment. i'm glad that the director chose to make this change as nobody wants to see a remake that directly copies of the original; plus there's the fact that the location is well used and always nice to look at. unfortunately, however, the positive elements of sleuth 2007 end there.the original film was over two hours long, while this remake is only just a shade over eighty minutes. naturally, therefore, that means that this version has less about it; and unfortunately it's the characters that suffer. the plot is also rushed and we get into the first twist in the tale far too quickly and before we are given any chance to actually understand why and how these events can be taking place. the film does not build the characters, or the rapport between them, enough to make sure that their relationship makes sense. one major thing that has been changed about the older character is his obsession; in the original he was obsessed with games which turned out to be very important once the twists come into play. here he has some kind of security fetish that doesn't really mean anything by the end. kenneth branagh's handling of the film allows for a classy score but the class ends there. the original thrived on it, but this film is happy merely to soil itself with expletives on numerous, and mostly unwarranted, occasions; which cheapen the whole thing. the final twist in the tale is completely different to how it was in the original and ensures that the film boils down to a really hideous conclusion. after spending two hours with the original i understood, respected and liked both characters presented in the film - after eighty minutes of this, i hated them both. i do have some respect for branagh for not merely rolling out a carbon copy of the original film; but this is not a good adaptation of the great anthony shaffer play. bin_features [0 0 1 1 1 1 0 1 0 0 1 1 0 0 1]\n",
      "text sample {7} \"bruce almighty\" looks and sounds incredibly stupid, especially from the trailers. nevertheless, i found in it a deeper message that actually made me like this film more. bruce (jim carrey) is angry at god and is given divine powers by him to be god for a week to see if he can do a better job. morgan freeman plays a man symbolized here as god, and though it isn't his usual type of film or one of his best roles, he does excellent with what he is given to work with. although crude at times, the film does have quite a few laughs, from bruce parting his soup in half like the red sea and the customers' reactions to him, as well as freeman's seemingly laid-back and wisecracking image of god. it is overly exaggerated at times, and there is some crude humor, but overall it manages to be somewhat funny. there is a decent supporting cast, such as jennifer aniston, lisa ann walter, and steve carrell, which always helps. the end of the film proves to be very romantic and tear-jerking, and the message is clear, that we should do what god has called us to do and \"be the miracle.\" the film is far from perfect, but still enjoyable, and far better than i and many people probably would have expected, especially if we see the deeper message of the film.*** out of **** bin_features [0 0 1 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      "text sample {8} excellent writing and wild cast. the tech is poor but it's obviously very low budget. looks like they didn't cut the negative but had to release on a video output. in any case one of the most inventive comedies i've seen lately. the screenwriter in particular is fine. bin_features [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "text sample {9} louis sachar's compelling children's classic is about as disney as freddy krueger. it's got murder, racism, facial disfigurement and killer lizards.tightly plotted, it's a multi-layered, interlinking story that spans history to reveal stanley's own heritage and the secret behind the holes. it races from latvia's lush greenness to the pock-marked camp green lake (hint: there's no lake and no green).disney's first success is re-creating the novel's environments so convincingly - the set design is superb and without gloss. the other plus is in the casting. rising star shia labeouf (charlie's angels 2, project greenlight) might not be the fat boy of the book, but his attitude is right and he's far from the usual clean-cut hero. the rest of the cast is filled out equally well, from patricia arquette as the frontier school marm-turned-bank robber to henry winkler as stanley's dad. the downside is the pop soundtrack - pure marketing department - and having the sentiment turned up to full volume at the end. bin_features [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "text sample {10} i am commenting on this miniseries from the perspective of someone who read the novel first. and from that perspective i can honestly say that while enjoyable, i can see why it hasn't been rebroadcast anytime recently. more specifically, this mini has some serious problems, such as:1) it is terribly miscast. the actors who played the younger generation were all 15 to 20 years older than the characters. ali mcgraw (45 at the time) was playing natalie jastrow who was supposed to be about 26. jan-michael vincent (39 at the time) was playing byron henry who was supposed to be about 22. the other henry children, and pamela tudsbury, were also played by actors way too old for characters who were supposed to be in their 20's.2) some of the acting was absolutely awful. ali mcgraw at times almost made this mini unwatchable. i have seen more convincing performances in high school plays. 3) the directing was poor. to be fair to ali mcgraw, the bad acting and character development were probably the directing. the portrayal of hitler was way overdone. his character came off looking and behaving more like a cartoon villain than the charismatic, sometimes charming, but always diabolical genius herman wouk painted him as in the novel. some of the other characters are done so stereotypically (berel jastrow) they do not gain the depth of character that wouk created for them.4) this mini is very dated. the hokey music, the pretentious narration (it sounded like a junior high school history film narration), and the entire prime-time soap opera feel of the mini made it almost comical at times. also, too often byron and natalie are costumed and made up to look like they are in 1979 rather than 1939.someone who watches this without the benefit of reading the novel first will probably not sit through it all, because it will come off more as a late 70's / early 80's \"take myself too seriously\" prime-time soap drama, rather than the television version of what is certainly a modern american classic.remakes of older movies and the like are sometimes poorly done, but this is probably one case where a creative and inspired director could make a very stunning, memorable, and critically acclaimed production. i don't ever see that happening since a remake would have to be just as long (15 hours) or longer to do it right, and given the short attention span of most of the current american viewing public, it wouldn't fly. bin_features [1 1 1 0 0 1 1 1 0 0 0 0 1 0 1]\n",
      "text sample {11} the bourne ultimatum (2007) review: after a thrilling set of two, we get the final installment. here's my take:the bourne ultimatum has it all. we have jason bourne(matt damon) on the coattails of the ones who know everything. he has been running for too long. this time, it ends.the bourne ultimatum has a great plot, awesome writing, fantastic direction, suspense, and some of the best action of the summer. matt damon delivers possibly his best performance to date. he has the conviction and swelling desire of the troubled assassin.there are some intelligent humor here and some fine suspense. the reactions to certain events will have you either laughing(in a good way) or cheering on. (or both) i heard a lot of intelligent laughter in the theater and lots of clapping. the audience was loving it.the bourne ultimatum delivers all in a nicely gift-wrapped package. all of the goods and then some. this is, in my opinion the best movie this summer.the last word: excellent conclusion. the best of the trilogy. this is how summer movie thrillers should be done. i love the bourne trilogy. bin_features [0 0 0 1 1 0 0 0 1 0 0 0 1 0 1]\n",
      "text sample {12} brothers with psychokinetic powers (yes, really) duel not just for debra winger's affections but really over a secret from their childhood that left them at odds over their powers.there are surreal touches (the fire brigade that act like a singing greek chorus), but there is also humour and romance. the soundtrack is great similar to the way american werewolf in london used every great wolf song they could get ~ but with fire ~ and i don't think i'll ever forget dennis quaid (mmmmm dennis quaid), setting his own trailer a rockin' to 'she's a lady' ~ priceless ;)best line missing from the quotes section btw ~ 'once you've had a clown, you never go back!'i love this movie (i ordered the dvd from the us) and if the comments written by the kind of people who'd be happier with legally blond 3 don't put you off ~ give it a try :) bin_features [0 1 0 0 1 1 1 0 1 1 1 0 0 0 1]\n",
      "text sample {13} one of the greatest film i have seen this year.last maybe before sun rise, which is also seen late at night alone in the lab. i like the idea of the film,which suggest free will of man and our weakness against fate.with time past by james and kathryn are destined to fail and an indescribable sorrow comes. i do like the end. but a big question also comes. the virus shall not be released again, should it?in the last scene in the airport. jose is sent back to meet james again by future scientists. when he tell him that scientists had already got his message and know someone else would spread the virus. and they two together meet kathryn when kathryn tell james the true man is dr. goines assistant. so it is clearly jose also get the true information about the virus,(james keep an eye on him at the time remember?) and he has teeth. so why everything is still happen?? why future scientists don't do anything after the truth is revealed?? my biggest question after the film... bin_features [0 1 1 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      "text sample {14} if you want to be cynical and pedantic you could point out that the opening where a raf lancaster bomber is mortally wounded on the 2nd of may 1945 is somewhat unlikely since german air defences were as lively as adolph hitler on that day but this isn't a movie that should be viewed by a cynical audience and i guess a character being killed in literally the last hours of the war adds to the poignancy . in fact you'd have to have survived the second world war to fully appreciate the intellect , beauty and soul of powell and pressburger's masterpiece . the scenes of heaven are painfully twee when viewed today ? again you have to view the movie of the context when it was made . raf bomber command lost 58,000 men during the war , the same number that america lost in 'nam but during a shorter period and a far , far smaller pool of active combatants , there's no atheists in a fox hole and i doubt if you'd lost a relative during the conflict you'd view material atheism as being a sensible thing . when richar attenborough's young pilot looks down in awe at the sight below him many war heroes must have openly wept at this scene as they remembered much missed comrades who didn't survive the war . also bare in mind that despite losing several million people from 1939-45 there seems to be very few people from germany passing through the pearly gates . it's obvious nazis don't go to heaven the plot itself where dashing young pilot peter carter arguing for his life in front of a celestial court wouldn't have had much appeal to me if it wasn't for the subtext , you see a matter or life and death is a highly political and visionary film that laments the end of the british empire as it's replaced by american ambitions . there's little things that show up the film as being made by people aware of american history and culture . one is the ethnic mix of america , even today many britons think that the usa is overwhelmingly composed of white anglo saxon protestants when in fact only 51% of americans are \" white european \" . the film rightly contains a scene where a multitude of different races confess \" i am an american \" as peter is judged by abraham farlan , an anglophobe who was the first revolutionary killed by british forces in the american war of independence . as for the \" special relationship \" between britain and america - what special relationship ? powell and pressburger know their history when it comes to britain and america . they obviously know their future too so remember to watch this movie with some of your mind in the past and some of your mind in the present . it's strange , beautiful , poignant and clever but most of all it's a film that would never ever work if it were made in the last 40 years . can you imagine if the story was set in 2003 and revolved around a british soldier killed in iraq ? bin_features [0 1 1 0 0 0 0 0 1 1 0 1 0 1 0]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(\n",
    "    num_samples=10,\n",
    "    min_df=1,\n",
    "    max_features=15,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=30,\n",
    "    shots=None,\n",
    "    n_test_explanations=10,\n",
    "    stop_words = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load data with given params (includes text cleaning)\n",
    "    2) Train logistic classifier\n",
    "    3) Evaluate test accuracy\n",
    "    4) Pick n_test_explanations random samples\n",
    "    5) For each, run classical LIME vs. Q-LIME Pi\n",
    "    6) Return summary stats\n",
    "    \"\"\"\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option,\n",
    "        stop_words = stop_words\n",
    "    )\n",
    "    # B) Train model\n",
    "    \n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "    sample_indices = [0,1,2,3,4,5,6,7,8,9, 10, 11, 12,13,14]\n",
    "    #random.sample(range(n_test), n_test_explanations)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "        y_true = y_test[idx]\n",
    "\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "        #print(\"X_test_bow\",X_test_bow)\n",
    "          \n",
    "        print(\"text sample\",{idx}, text_sample, \"bin_features\", bin_features)\n",
    "        #, \"vectorizer\", vectorizer.get_feature_names_out(), \"contributions_qlime_abs\", contributions_qlime_abs, \"Contributions_Lime\", top_words_lime\n",
    "     \n",
    "print(run_experiment(num_samples=5000, min_df=1, max_features=15, stopwords_option=True, lime_num_samples=30, shots=None, n_test_explanations=100, stop_words = 'english'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
