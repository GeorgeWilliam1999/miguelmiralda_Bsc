{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classical LIME\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0: DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove anything between <...> tags, then lowercase the text\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=5000, \n",
    "    min_df=1, \n",
    "    max_features=15, \n",
    "    stopwords_option=True,\n",
    "    stop_words = 'english'\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \n",
    "    Now with text cleaning for HTML, lowercase, etc.\n",
    "    \"\"\"\n",
    "    data = load_files(\n",
    "        \"C:/Users/migue/Downloads/aclImdb_v1/aclImdb/train\",\n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"                  \n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "    # Clean text (HTML removal + lowercase)\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    #np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=stop_words, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=None, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "#def train_XGBoost_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    # Use log(len(y_train)) as n_estimators (rounded to an int)\n",
    "    clXGB = XGBClassifier(\n",
    "        #booster=\"gblinear\",\n",
    "        objective=\"binary:logistic\", \n",
    "        eval_metric=\"logloss\", \n",
    "        random_state=42, \n",
    "        n_estimators=int(round(math.log(len(y_train)))),\n",
    "        learning_rate=0.1, \n",
    "        max_depth=3\n",
    "    )\n",
    "    clXGB.fit(X_train_bow, y_train)\n",
    "    return clXGB\n",
    "\n",
    "#def get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_xgboost_ns{num_samples}_mf{max_features}_sw{stopwords_option}_xgboost_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached xgboost from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clXGB = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clXGB = train_XGBoost_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clXGB, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clXGB\n",
    "def train_NN_classifier(X_train, y_train, X_test, y_test, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a neural network on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    X_valid_bow = vectorizer.transform(X_test)\n",
    "    input_dim = X_train_bow.shape[1]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),  # First hidden layer\n",
    "        Dropout(0.3),  # Dropout with 30% probability\n",
    "        Dense(32, activation='relu'),  # Second hidden layer\n",
    "        Dropout(0.2),  # Dropout with 20% probability\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_bow, y_train, epochs=100, batch_size=10, validation_data=(X_valid_bow, y_test), verbose=1)\n",
    "    return model\n",
    "\n",
    "def get_cached_NN(X_train, y_train, vectorizer, num_samples, max_features, stop_words, X_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_classifier_ns{num_samples}_mf{max_features}_sw{stop_words}_NN_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached logistic from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clNN = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clNN = train_NN_classifier(X_train, y_train, X_valid, y_valid, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clNN, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clNN\n",
    "\n",
    "#def train_logistic_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    return clf\n",
    "\n",
    "#def get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_classifier_ns{num_samples}_mf{max_features}_sw{stop_words}_logistic_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached logistic from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clf\n",
    "\n",
    "#def train_lasso_regression(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    lasso_model = Lasso(alpha=0.5)\n",
    "    lasso_model.fit(X_train_bow, y_train)\n",
    "    return lasso_model\n",
    "    \n",
    "\n",
    "#def get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha):\n",
    "    \"\"\"\n",
    "    Checks if a Lasso model trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_lasso_ns{num_samples}_mf{max_features}_sw{stopwords_option}_seed42_alpha{alpha}.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached Lasso model from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            lasso_model = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached Lasso model found. Training a new one...\")\n",
    "        lasso_model = train_lasso_regression(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(lasso_model, f)\n",
    "        print(\"Cached Lasso model saved as\", filename)\n",
    "    return lasso_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSICAL LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#CHANGE clXGB TO clf IF WE WANT LOGISTIC INSTEAD OF XGBOOST\n",
    "def run_classical_lime(\n",
    "    text_sample, clXGB, vectorizer,  # These values dont really matter, \n",
    "    k_features=10, num_samples=500   # The values used re the ones used when u call the function later\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns the top (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names, feature_selection=\"auto\")\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts) \n",
    "        proba = clXGB.predict(bow)\n",
    "        return np.hstack((1 - proba, proba))  # Return probabilities for both classes\n",
    "        \n",
    "        \n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features,\n",
    "        num_samples=num_samples  # e.g. 300 or 500\n",
    "    )\n",
    "    return explanation.as_list()  # list of (word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LIME Pi (Flip Only 1->0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL ROUTINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment( #Did I change these numbers? check if i fcked up smth here!!!!!!!!!!\n",
    "    num_samples=10,\n",
    "    min_df=1,\n",
    "    max_features=15,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=30,\n",
    "    shots=None,\n",
    "    n_test_explanations=10,\n",
    "    stop_words = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load data with given params (includes text cleaning)\n",
    "    2) Train logistic classifier\n",
    "    3) Evaluate test accuracy\n",
    "    4) Pick n_test_explanations random samples\n",
    "    5) For each, run classical LIME vs. Q-LIME Pi\n",
    "    6) Return summary stats\n",
    "    \"\"\"\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option,\n",
    "        stop_words = stop_words\n",
    "    )\n",
    "    # B) Train model\n",
    "\n",
    "    #clf  = get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "    #clXGB = get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "    clNN = get_cached_NN(X_train, y_train, vectorizer, num_samples, max_features, stop_words, X_test, y_test)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    test_acc = accuracy_score(y_test, clNN.predict(X_test_bow) > 0.5)  # Convert probabilities to binary class labels\n",
    "    #test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    #test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    #logistic_weights = clf.coef_[0]\n",
    "    #bias = clf.intercept_[0]\n",
    "\n",
    "\n",
    "    # IT ONLY GIVES 1 WEIGHT NOT 15 !!!!\n",
    "    #logistic_weights = clXGB.coef_[0]\n",
    "    #bias = clXGB.intercept_[0]\n",
    "\n",
    "\n",
    "    #lasso_model = get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha=0.1)\n",
    "    \n",
    "\n",
    "    # We'll track times & top-feature overlap\n",
    "    lime_times = []\n",
    "    # qlime_times = []\n",
    "    # overlaps = []\n",
    "    instance_local_accuracies = []\n",
    "\n",
    "    # Random samples for explanation\n",
    "    #n_test = len(X_test)\n",
    "    sample_indices = [5,6,12,11,10, 0, 1, 2, 3, 4]\n",
    "    #random.sample(range(n_test), n_test_explanations)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "        y_true = y_test[idx]\n",
    "\n",
    "        # 1) Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(\n",
    "            text_sample, clNN, vectorizer, \n",
    "            k_features=15, num_samples=lime_num_samples\n",
    "        )\n",
    "\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        y_pred = clNN.predict(bow)[0]\n",
    "        instance_accuracy = int(y_pred == y_true)\n",
    "        instance_local_accuracies.append(instance_accuracy)\n",
    "\n",
    "        #explanation_lime = run_classical_lime(\n",
    "        #    text_sample, clf, vectorizer, \n",
    "        #    k_features=15, num_samples=lime_num_samples\n",
    "        #)\n",
    "        lime_time = time.time() - start_lime\n",
    "        lime_times.append(lime_time)\n",
    "\n",
    "        # parse top features\n",
    "        lime_dict = dict(explanation_lime)\n",
    "        top_words_lime = sorted(\n",
    "            lime_dict.keys(),\n",
    "            key=lambda w: abs(lime_dict[w]),\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        # 2) Q-LIME Pi\n",
    "        \n",
    "\n",
    "        #start_qlime = time.time()\n",
    "        #contributions_qlime = quantum_lime_explanation(bin_features, clf, lasso_model, shots=shots)\n",
    "        #contributions_qlime = quantum_lime_explanation(\n",
    "        #    bin_features, logistic_weights, bias=bias, shots=shots)\n",
    "\n",
    "\n",
    "        contributions_lime_abs = [(word, abs(score)) for word, score in explanation_lime] # Absolute values for comparison; This is a tuple. PROB SHOULD MAKE QLIME A TUPLE TOO!\n",
    "        \n",
    "        #unsorted_contributions_qlime_abs = tuple(\n",
    "            #(word, abs(score)) for word, score in zip(vectorizer.get_feature_names_out(), contributions_qlime)) # Absolute values for comparison\n",
    "        \n",
    "        #contributions_qlime_sorted = tuple(\n",
    "        #sorted(unsorted_contributions_qlime_abs, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        #print(\"X_test_bow\",X_test_bow)\n",
    "          \n",
    "        print(\"text sample\", text_sample, \"bin_features\", bin_features)\n",
    "        #, \"vectorizer\", vectorizer.get_feature_names_out(), \"contributions_qlime_abs\", contributions_qlime_abs, \"Contributions_Lime\", top_words_lime\n",
    "     \n",
    "        print(\"Classical LIME Explanation:\")\n",
    "        for word, weight in contributions_lime_abs:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "\n",
    "        # print(\"\\nQ-LIME Pi Explanation:\")\n",
    "        #for word, weight in contributions_qlime_sorted:\n",
    "        #    print(f\"Word: {word}, Importance: {weight}\")\n",
    "        \n",
    "        #print(\"\\n weights\", clf.coef_[0])\n",
    "        #qlime_time = time.time() - start_qlime\n",
    "        #qlime_times.append(qlime_time)\n",
    "\n",
    "        # top 5 (by absolute value)\n",
    "        # nonzero_indices = [\n",
    "        #     (i, abs(contributions_qlime[i])) \n",
    "        #     for i in range(len(contributions_qlime))\n",
    "        # ] \n",
    "        # top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "        # top_words_qlime = [\n",
    "        #     vectorizer.get_feature_names_out()[i2]\n",
    "        #     for (i2, val) in top_indices_qlime\n",
    "        # ] \n",
    "\n",
    "        # # measure overlap\n",
    "        #  overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "        # overlaps.append(len(overlap)) \n",
    "\n",
    "    # Summary\n",
    "    results = {\n",
    "        \"local_accuracy\": np.mean(instance_local_accuracies),\n",
    "        \"lime_time_avg\": round(np.mean(lime_times), 4),\n",
    "        # \"qlime_time_avg\": round(np.mean(qlime_times), 4),\n",
    "        # \"overlap_avg\": round(np.mean(overlaps), 4),\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=20, stopwords=True, lime_num_samples=500, shots=100,stop_words=english,n_test_explanations=5\n",
      "Loading cached logistic from cached_classifier_ns500_mf20_swenglish_NN_classifier_seed42.pkl\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "text sample having been driven out of the house and into the theater by the sweltering heat, i could not have been more pleased. the road to perdition, directed by sam mendes (american beauty), is destined to become one of the greatest movies of all time. perhaps i'm just getting old; perhaps i've just seen the same themes recycled time and again. but this movie is indeed different.the story opens with young michael sullivan jr. facing out to the sea, contemplating the duality of his father's legacy -- one of the best men to ever live, one of the most evil. this duality snakes its way throughout the movie. the story revolves around crime boss john rooney (paul newman) and michael sullivan (tom hanks), the young man rooney once took in and who now serves as his personal \"angel of death.\" rooney is tied by blood to his own son, but tied by love and loyalty to michael. young michael jr., intrigued by the stories he reads, steals away in his father's car one night while dad goes off to \"work\" with connor rooney, heir to the family \"business.\" connor lets the situation get out of hand, and what was meant only to be a warning turns into murder -- witnessed by michael jr. upon the discovery that young michael has seen what he should not have seen, the plot is set in motion as conflicting loyalties collide. soon, michael sr. is on the run with his young son, pursued by contract killer harlen \"the reporter\" maguire (jude law).i will disclose no further details in order to avoid any potential spoilers. however, i strongly encourage viewers to examine the many dualities that present themselves in the movie: problems between sons and fathers (michael sr & jr., john rooney & son connor), between the world at home and the world at \"work\", between good and evil, between those who pretend to be men of god and those who really are, between \"clean\" money and \"dirty\", between the town of perdition and perdition as hell. and along the way, savor the visual brilliance of cinematographer conrad l. hall (9 nominations, 2 oscars for best cinematography): rain pouring off fedoras, shots through mirrors (especially on swinging doors), tommy-gun flashes from out of the shadows, absent any sound. not only has 75-year-old hall given us perhaps the best cinematic product of his career, but 77-year-old paul newman offers one of his best performances ever.yes ... i may be getting old. but i've seen a lot ... and this is fresh and invigorating. the road to perdition presents a lasting and loving tribute to the gangster genre, to films of the 40s, to dark comic-book figures lurking in the darkness, to villains and heroes, to american film in general. go see it! bin_features [0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: plot, Importance: 0.33301511842435233\n",
      "Word: just, Importance: 0.24613271944457665\n",
      "Word: way, Importance: 0.2245547074301717\n",
      "Word: best, Importance: 0.21062127354254187\n",
      "Word: movies, Importance: 0.18368342994025386\n",
      "Word: movie, Importance: 0.15765671007894988\n",
      "Word: good, Importance: 0.13413046879996862\n",
      "Word: on, Importance: 0.08096507616424309\n",
      "Word: really, Importance: 0.06733995547729016\n",
      "Word: dirty, Importance: 0.05366553741612623\n",
      "Word: examine, Importance: 0.03783122197354466\n",
      "Word: cinematic, Importance: 0.03697639249884369\n",
      "Word: steals, Importance: 0.03520980166677712\n",
      "Word: films, Importance: 0.006395200223798994\n",
      "Word: heir, Importance: 0.005063599155995002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_9384\\1719213264.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  instance_accuracy = int(y_pred == y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "text sample this is so incredibly bad. poor actors. you can tell they're trying really hard to polish a turd, but we all know you can't. the writing is so obvious and facile, it's sad watching them try to sell it. the humor and pacing are so labored, it's hard to believe any of these good actors signed on for this.that said, it's so awful that we're having a hard time looking away from the screen. we just have to know where this trainwreck goes. but that's only because we caught it on tv. if we had actually paid for this, we'd be disgusted. so it gets 2 stars for being at least amusingly/fascinatingly bad. and the incidental music (as opposed to the trying-too-hard indie soundtrack) is laughably reminiscent of an episode of scooby-doo... but not as good. bin_features [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: really, Importance: 0.38486423048740553\n",
      "Word: time, Importance: 0.2909532261971409\n",
      "Word: just, Importance: 0.22888058224495245\n",
      "Word: bad, Importance: 0.2019989407339676\n",
      "Word: tell, Importance: 0.06503753658632201\n",
      "Word: all, Importance: 0.06173248800590195\n",
      "Word: signed, Importance: 0.054791243158520075\n",
      "Word: is, Importance: 0.05440833085462466\n",
      "Word: good, Importance: 0.04975677945893022\n",
      "Word: amusingly, Importance: 0.04241247952868852\n",
      "Word: these, Importance: 0.039938638307350005\n",
      "Word: watching, Importance: 0.03785710609473454\n",
      "Word: incidental, Importance: 0.036706673590043384\n",
      "Word: trying, Importance: 0.02123366295001971\n",
      "Word: incredibly, Importance: 0.016187630293006648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_9384\\1719213264.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  instance_accuracy = int(y_pred == y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "text sample i chanced upon this movie because i had a free non-new release from blockbuster and needed to grab something quickly, as the store was getting ready to close for the evening. the plain white cover and title intrigued me. i'm a (relatively speaking) \"old\" lady and my son is a young man of 30. i adore movies that are sheer entertainment, such as the sixth sense, interview with a vampire, harry potter and beetlejuice. my son, on the other hand, is a film graduate and enjoys very specialized foreign films, such as those directed by bergman or hertzog. we generally hate each other's movie choices, however, we both watched and loved the movie nothing! it was unlike any movie we'd ever seen before. we're both cynical/critical personality types and we usually crack on movies while we watch them -- but in this case we just laughed and enjoyed the film from start to finish. it is our opinion that if this movie had been promoted and shown in the main stream theaters in the u.s. it would have done very well indeed. bin_features [0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: movies, Importance: 0.356470963895469\n",
      "Word: watch, Importance: 0.1550946277271643\n",
      "Word: just, Importance: 0.08889352101647534\n",
      "Word: movie, Importance: 0.06774745855415275\n",
      "Word: me, Importance: 0.06713943537202133\n",
      "Word: something, Importance: 0.06325490780786835\n",
      "Word: speaking, Importance: 0.057031759327300736\n",
      "Word: relatively, Importance: 0.041923749548965904\n",
      "Word: sheer, Importance: 0.03849730850570991\n",
      "Word: m, Importance: 0.03610591615910635\n",
      "Word: it, Importance: 0.030501794269475333\n",
      "Word: a, Importance: 0.03030357803499814\n",
      "Word: was, Importance: 0.028784194098139863\n",
      "Word: non, Importance: 0.022061565910831714\n",
      "Word: cynical, Importance: 0.018112703815085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_9384\\1719213264.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  instance_accuracy = int(y_pred == y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "text sample although the plot was a bit sappy at times, and very rushed at the end, as if the director had run out of his alloted time and needed to hurry up and finish the story, overall it was pretty good for the made-for-backwoods-cable-tv genre. however, the actress who played the babysitter, mariana klaveno, was very good! i hope to see more of her around in movie-land. the music was also well done, getting every possible chill out of the dah-duh-dah-duh (think \"jaws\") type music-based tension build-ups.i don't think i'd want to watch \"while the children sleep\" again, but if i did, it would be to focus on the performance of the talented klaveno. bin_features [0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: don, Importance: 0.2698347607147438\n",
      "Word: plot, Importance: 0.20520979415611257\n",
      "Word: watch, Importance: 0.18734487425960003\n",
      "Word: movie, Importance: 0.14441845469674555\n",
      "Word: needed, Importance: 0.06421878048243523\n",
      "Word: time, Importance: 0.06338991040324242\n",
      "Word: done, Importance: 0.042569595403739065\n",
      "Word: who, Importance: 0.038359731891109895\n",
      "Word: did, Importance: 0.03744117749157237\n",
      "Word: cable, Importance: 0.03710243959297047\n",
      "Word: based, Importance: 0.036424384654625816\n",
      "Word: was, Importance: 0.033522678629871246\n",
      "Word: tv, Importance: 0.03039931358441239\n",
      "Word: rushed, Importance: 0.02620612561336615\n",
      "Word: i, Importance: 0.02041254285597249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_9384\\1719213264.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  instance_accuracy = int(y_pred == y_true)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/16\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 82ms/step"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1654 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[17] = 497 is not in [0, 497)\n\t [[{{node RaggedGather/RaggedGather}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 32\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment with: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_features=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_features_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_words=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop_words_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_test_explanations=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_test_explanations_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m res \u001b[38;5;241m=\u001b[39m run_experiment(\n\u001b[0;32m     33\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39mnum_samples_,\n\u001b[0;32m     34\u001b[0m     max_features\u001b[38;5;241m=\u001b[39mmax_features_,\n\u001b[0;32m     35\u001b[0m     stopwords_option\u001b[38;5;241m=\u001b[39mstopwords_,\n\u001b[0;32m     36\u001b[0m     lime_num_samples\u001b[38;5;241m=\u001b[39mlime_samps_,\n\u001b[0;32m     37\u001b[0m     shots\u001b[38;5;241m=\u001b[39mshots_,\n\u001b[0;32m     38\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39mstop_words_,\n\u001b[0;32m     39\u001b[0m     n_test_explanations\u001b[38;5;241m=\u001b[39mn_test_explanations_,\n\u001b[0;32m     40\u001b[0m     \n\u001b[0;32m     41\u001b[0m     \n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m res_row \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_samples_,\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_features_,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_words\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_words_\n\u001b[0;32m     55\u001b[0m }\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults =>\u001b[39m\u001b[38;5;124m\"\u001b[39m, res_row)\n",
      "Cell \u001b[1;32mIn[104], line 68\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(num_samples, min_df, max_features, stopwords_option, lime_num_samples, shots, n_test_explanations, stop_words)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# 1) Classical LIME\u001b[39;00m\n\u001b[0;32m     67\u001b[0m start_lime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 68\u001b[0m explanation_lime \u001b[38;5;241m=\u001b[39m run_classical_lime(\n\u001b[0;32m     69\u001b[0m     text_sample, clNN, vectorizer, \n\u001b[0;32m     70\u001b[0m     k_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39mlime_num_samples\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     73\u001b[0m bow \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([text_sample])\n\u001b[0;32m     74\u001b[0m bin_features \u001b[38;5;241m=\u001b[39m bow\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[103], line 20\u001b[0m, in \u001b[0;36mrun_classical_lime\u001b[1;34m(text_sample, clXGB, vectorizer, k_features, num_samples)\u001b[0m\n\u001b[0;32m     15\u001b[0m     proba \u001b[38;5;241m=\u001b[39m clXGB\u001b[38;5;241m.\u001b[39mpredict(bow)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m proba, proba))  \u001b[38;5;66;03m# Return probabilities for both classes\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m explanation \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(\n\u001b[0;32m     21\u001b[0m     text_sample,\n\u001b[0;32m     22\u001b[0m     predict_proba,\n\u001b[0;32m     23\u001b[0m     num_features\u001b[38;5;241m=\u001b[39mk_features,\n\u001b[0;32m     24\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39mnum_samples  \u001b[38;5;66;03m# e.g. 300 or 500\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m explanation\u001b[38;5;241m.\u001b[39mas_list()\n",
      "File \u001b[1;32mc:\\Users\\migue\\miniconda\\Lib\\site-packages\\lime\\lime_text.py:413\u001b[0m, in \u001b[0;36mLimeTextExplainer.explain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    406\u001b[0m indexed_string \u001b[38;5;241m=\u001b[39m (IndexedCharacters(\n\u001b[0;32m    407\u001b[0m     text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow, mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string)\n\u001b[0;32m    408\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m    409\u001b[0m                   IndexedString(text_instance, bow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbow,\n\u001b[0;32m    410\u001b[0m                                 split_expression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_expression,\n\u001b[0;32m    411\u001b[0m                                 mask_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_string))\n\u001b[0;32m    412\u001b[0m domain_mapper \u001b[38;5;241m=\u001b[39m TextDomainMapper(indexed_string)\n\u001b[1;32m--> 413\u001b[0m data, yss, distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_labels_distances(\n\u001b[0;32m    414\u001b[0m     indexed_string, classifier_fn, num_samples,\n\u001b[0;32m    415\u001b[0m     distance_metric\u001b[38;5;241m=\u001b[39mdistance_metric)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(yss[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n",
      "File \u001b[1;32mc:\\Users\\migue\\miniconda\\Lib\\site-packages\\lime\\lime_text.py:482\u001b[0m, in \u001b[0;36mLimeTextExplainer.__data_labels_distances\u001b[1;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[0;32m    480\u001b[0m     data[i, inactive] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    481\u001b[0m     inverse_data\u001b[38;5;241m.\u001b[39mappend(indexed_string\u001b[38;5;241m.\u001b[39minverse_removing(inactive))\n\u001b[1;32m--> 482\u001b[0m labels \u001b[38;5;241m=\u001b[39m classifier_fn(inverse_data)\n\u001b[0;32m    483\u001b[0m distances \u001b[38;5;241m=\u001b[39m distance_fn(sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix(data))\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, labels, distances\n",
      "Cell \u001b[1;32mIn[103], line 15\u001b[0m, in \u001b[0;36mrun_classical_lime.<locals>.predict_proba\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(texts):\n\u001b[0;32m     14\u001b[0m     bow \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(texts) \n\u001b[1;32m---> 15\u001b[0m     proba \u001b[38;5;241m=\u001b[39m clXGB\u001b[38;5;241m.\u001b[39mpredict(bow)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m proba, proba))\n",
      "File \u001b[1;32mc:\\Users\\migue\\miniconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\migue\\miniconda\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} Error in user-defined function passed to ParallelMapDatasetV2:1654 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2: indices[17] = 497 is not in [0, 497)\n\t [[{{node RaggedGather/RaggedGather}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # Parameter grid to systematically vary certain settings\n",
    "    param_grid = {\n",
    "        \"num_samples\": [500],\n",
    "        \"max_features\": [20],\n",
    "        \"stopwords_option\": [True],\n",
    "        \"lime_num_samples\": [500],\n",
    "        # Shots: None => analytic mode, 100 => finite sampling\n",
    "        \"shots\": [100],\n",
    "        \"stop_words\": ['english'],\n",
    "        \"n_test_explanations\": [5]\n",
    "    }\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    all_results = []\n",
    "\n",
    "    for combo in combos:\n",
    "        (num_samples_, max_features_, stopwords_, lime_samps_, shots_, stop_words_, n_test_explanations_) = combo\n",
    "        \n",
    "        print(\"\\n==================================\")\n",
    "        print(f\"Running experiment with: \"\n",
    "              f\"num_samples={num_samples_}, \"\n",
    "              f\"max_features={max_features_}, \"\n",
    "              f\"stopwords={stopwords_}, \"\n",
    "              f\"lime_num_samples={lime_samps_}, \"\n",
    "              f\"shots={shots_},\"\n",
    "              f\"stop_words={stop_words_},\"\n",
    "              f\"n_test_explanations={n_test_explanations_}\")\n",
    "        \n",
    "        res = run_experiment(\n",
    "            num_samples=num_samples_,\n",
    "            max_features=max_features_,\n",
    "            stopwords_option=stopwords_,\n",
    "            lime_num_samples=lime_samps_,\n",
    "            shots=shots_,\n",
    "            stop_words=stop_words_,\n",
    "            n_test_explanations=n_test_explanations_,\n",
    "            \n",
    "            \n",
    "        )\n",
    "        res_row = {\n",
    "            \"num_samples\": num_samples_,\n",
    "            \"max_features\": max_features_,\n",
    "            \"stopwords\": stopwords_,\n",
    "            \"lime_num_samples\": lime_samps_,\n",
    "            \"shots\": shots_,\n",
    "            \"local_accuracy\": res[\"local_accuracy\"],\n",
    "            \"lime_time_avg\": res[\"lime_time_avg\"],\n",
    "            # \"qlime_time_avg\": res[\"qlime_time_avg\"],\n",
    "            # \"overlap_avg\": res[\"overlap_avg\"],\n",
    "            \"n_test_explanations\": n_test_explanations_,\n",
    "            \"stop_words\": stop_words_\n",
    "        }\n",
    "        print(\"Results =>\", res_row)\n",
    "        all_results.append(res_row)\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"results_expanded_flips.csv\", index=False)\n",
    "    print(\"\\nAll done! Saved results to 'results_expanded_flips.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
