{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classical LIME\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0: DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove anything between <...> tags, then lowercase the text\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=5000, \n",
    "    min_df=1, \n",
    "    max_features=15, \n",
    "    stopwords_option=True,\n",
    "    stop_words = 'english'\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \n",
    "    Now with text cleaning for HTML, lowercase, etc.\n",
    "    \"\"\"\n",
    "    data = load_files(\n",
    "        \"C:/Users/migue/Downloads/aclImdb_v1/aclImdb/train\",\n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"                  \n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "    # Clean text (HTML removal + lowercase)\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    #np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=stop_words, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=None, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "#def train_XGBoost_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    # Use log(len(y_train)) as n_estimators (rounded to an int)\n",
    "    clXGB = XGBClassifier(\n",
    "        #booster=\"gblinear\",\n",
    "        objective=\"binary:logistic\", \n",
    "        eval_metric=\"logloss\", \n",
    "        random_state=42, \n",
    "        n_estimators=int(round(math.log(len(y_train)))),\n",
    "        learning_rate=0.1, \n",
    "        max_depth=3\n",
    "    )\n",
    "    clXGB.fit(X_train_bow, y_train)\n",
    "    return clXGB\n",
    "\n",
    "#def get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_xgboost_ns{num_samples}_mf{max_features}_sw{stopwords_option}_xgboost_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached xgboost from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clXGB = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clXGB = train_XGBoost_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clXGB, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clXGB\n",
    "\n",
    "def train_logistic_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    return clf\n",
    "\n",
    "def get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_classifier_ns{num_samples}_mf{max_features}_sw{stop_words}_logistic_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached logistic from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clf\n",
    "\n",
    "#def train_lasso_regression(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    lasso_model = Lasso(alpha=0.5)\n",
    "    lasso_model.fit(X_train_bow, y_train)\n",
    "    return lasso_model\n",
    "    \n",
    "\n",
    "#def get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha):\n",
    "    \"\"\"\n",
    "    Checks if a Lasso model trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_lasso_ns{num_samples}_mf{max_features}_sw{stopwords_option}_seed42_alpha{alpha}.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached Lasso model from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            lasso_model = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached Lasso model found. Training a new one...\")\n",
    "        lasso_model = train_lasso_regression(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(lasso_model, f)\n",
    "        print(\"Cached Lasso model saved as\", filename)\n",
    "    return lasso_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSICAL LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#CHANGE clXGB TO clf IF WE WANT LOGISTIC INSTEAD OF XGBOOST\n",
    "def run_classical_lime(\n",
    "    text_sample, clXGB, vectorizer, \n",
    "    k_features=10, num_samples=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns the top (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names, feature_selection=\"auto\")\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts) \n",
    "        return clXGB.predict_proba(bow)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features,\n",
    "        num_samples=num_samples  # e.g. 300 or 500\n",
    "    )\n",
    "    return explanation.as_list()  # list of (word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LIME Pi (Flip Only 1->0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_classifier(features, weights, bias=0.0, threshold=0.01):\n",
    "    # Ensure inputs are 1D arrays\n",
    "    features = np.array(features).flatten()\n",
    "    weights = np.array(weights).flatten()\n",
    "    \n",
    "    # Zero out small weights\n",
    "    sparse_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
    "    score = bias + np.dot(features, sparse_weights)\n",
    "    return float(1 / (1 + np.exp(-score)))\n",
    "\n",
    "def encode_and_flip(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Encode features -> quantum circuit.\n",
    "    FLIP ONLY if bit == 1 at flip_index (1->0).\n",
    "    \"\"\"\n",
    "    num_qubits = len(features)\n",
    "    dev = qml.device(\"default.qubit\", wires=num_qubits, shots=shots)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit():\n",
    "        for i, f in enumerate(features):\n",
    "            if i == flip_index: # and f == 1:   ;;  This line is the original code commented out\n",
    "                # 1->0 => RY(0),\n",
    "                #theta = 0\n",
    "                #My suggestion: \n",
    "                theta = f * (np.pi / 2)            \n",
    "                qml.PauliX(wires=i)\n",
    "            else:\n",
    "                theta = f * (np.pi / 2)\n",
    " \n",
    "            qml.RY(theta, wires=i)\n",
    "            \n",
    "        return qml.probs(wires=range(num_qubits))\n",
    "\n",
    "    return circuit()\n",
    "\n",
    "def sample_state(probabilities):\n",
    "    \"\"\"\n",
    "    Sample an integer state index from the distribution.\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    cumsum = 0.0\n",
    "    for idx, p in enumerate(probabilities):\n",
    "        cumsum += p\n",
    "        if r <= cumsum:\n",
    "            return idx\n",
    "    return len(probabilities) - 1\n",
    "\n",
    "def measure_and_map_to_classical(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Run the circuit, measure, return a binary array for the top-likelihood state.\n",
    "    \"\"\"\n",
    "    probs = encode_and_flip(features, flip_index=flip_index, shots=shots)\n",
    "    measured_state = sample_state(probs)\n",
    "    num_qubits = len(features)\n",
    "    bin_string = f\"{measured_state:0{num_qubits}b}\"\n",
    "    return [int(bit) for bit in bin_string]\n",
    "\n",
    "\n",
    "def quantum_lime_explanation(\n",
    "    features, weights, bias=0.0, shots=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Flip only features that are 1 -> 0.\n",
    "    Return array of shape (n_features,) with:\n",
    "       Delta f_k = (original_pred - new_pred).\n",
    "    \"\"\"\n",
    "    original_pred = classical_classifier(features, weights, bias=bias)\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        new_vec = measure_and_map_to_classical(features, flip_index=i, shots=None)\n",
    "        new_pred = classical_classifier(new_vec, weights, bias=bias)\n",
    "        return original_pred - new_pred\n",
    "\n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) #if val == 1 # ;  This is the original code commented out \n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions\n",
    "\n",
    "#def quantum_lime_explanation(features, clf, lasso, shots=None):\n",
    "    \n",
    "    # Reshape features for prediction (ensure 2D array)\n",
    "    features_reshaped = np.array(features).reshape(1, -1)\n",
    "    original_pred = clf.predict_proba(features_reshaped)[0, 1]\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        # Create a new feature vector with feature i flipped from 1 to 0\n",
    "        new_vec = features.copy()\n",
    "        new_vec[i] = 0\n",
    "        new_vec_reshaped = np.array(new_vec).reshape(1, -1)\n",
    "        new_pred = lasso.predict(new_vec_reshaped)[0]\n",
    "        return original_pred - new_pred\n",
    " \n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) if val == 1\n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL ROUTINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment( #Did I change these numbers? check if i fcked up smth here!!!!!!!!!!\n",
    "    num_samples=10,\n",
    "    min_df=1,\n",
    "    max_features=15,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=30,\n",
    "    shots=None,\n",
    "    n_test_explanations=10,\n",
    "    stop_words = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load data with given params (includes text cleaning)\n",
    "    2) Train logistic classifier\n",
    "    3) Evaluate test accuracy\n",
    "    4) Pick n_test_explanations random samples\n",
    "    5) For each, run classical LIME vs. Q-LIME Pi\n",
    "    6) Return summary stats\n",
    "    \"\"\"\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option,\n",
    "        stop_words = stop_words\n",
    "    )\n",
    "    # B) Train model\n",
    "    clf  = get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "\n",
    "    #clXGB = get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    #test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    print(\"Global accuracy:\", test_acc) \n",
    "\n",
    "    logistic_weights = clf.coef_[0]\n",
    "    bias = clf.intercept_[0]\n",
    "\n",
    "\n",
    "    #IT ONLY GIVES 1 WEIGHT NOT 15\n",
    "    #logistic_weights = clXGB.coef_[0]\n",
    "    #bias = clXGB.intercept_[0]\n",
    "\n",
    "\n",
    "    #lasso_model = get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha=0.1)\n",
    "    \n",
    "\n",
    "    # We'll track times & top-feature overlap\n",
    "    lime_times = []\n",
    "    qlime_times = []\n",
    "    overlaps = []\n",
    "    instance_local_accuracies = []\n",
    "\n",
    "    # Random samples for explanation\n",
    "    #n_test = len(X_test)\n",
    "    sample_indices = [5, 6,12,11,10, 0, 1, 2, 3, 4]\n",
    "    #random.sample(range(n_test), n_test_explanations)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "        y_true = y_test[idx]\n",
    "\n",
    "        # 1) Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(\n",
    "            text_sample, clf, vectorizer, \n",
    "            k_features=15, num_samples=lime_num_samples\n",
    "        )\n",
    "\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        y_pred = clf.predict(bow)[0]\n",
    "        instance_accuracy = int(y_pred == y_true)\n",
    "        instance_local_accuracies.append(instance_accuracy)\n",
    "\n",
    "        #explanation_lime = run_classical_lime(\n",
    "        #    text_sample, clf, vectorizer, \n",
    "        #    k_features=15, num_samples=lime_num_samples\n",
    "        #)\n",
    "        lime_time = time.time() - start_lime\n",
    "        lime_times.append(lime_time)\n",
    "\n",
    "        # parse top features\n",
    "        lime_dict = dict(explanation_lime)\n",
    "        top_words_lime = sorted(\n",
    "            lime_dict.keys(),\n",
    "            key=lambda w: abs(lime_dict[w]),\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        # 2) Q-LIME Pi\n",
    "        \n",
    "\n",
    "        start_qlime = time.time()\n",
    "        #contributions_qlime = quantum_lime_explanation(bin_features, clf, lasso_model, shots=shots)\n",
    "        contributions_qlime = quantum_lime_explanation(\n",
    "            bin_features, logistic_weights, bias=bias, shots=shots)\n",
    "\n",
    "\n",
    "        contributions_lime_abs = [(word, abs(score)) for word, score in explanation_lime] # Absolute values for comparison; This is a tuple. PROB SHOULD MAKE QLIME A TUPLE TOO!\n",
    "        \n",
    "        unsorted_contributions_qlime_abs = tuple(\n",
    "            (word, abs(score)) for word, score in zip(vectorizer.get_feature_names_out(), contributions_qlime)) # Absolute values for comparison\n",
    "        \n",
    "        contributions_qlime_sorted = tuple(\n",
    "        sorted(unsorted_contributions_qlime_abs, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        #print(\"X_test_bow\",X_test_bow)\n",
    "          \n",
    "        print(\"text sample\", text_sample, \"bin_features\", bin_features)\n",
    "        #, \"vectorizer\", vectorizer.get_feature_names_out(), \"contributions_qlime_abs\", contributions_qlime_abs, \"Contributions_Lime\", top_words_lime\n",
    "     \n",
    "        print(\"Classical LIME Explanation:\")\n",
    "        for word, weight in contributions_lime_abs:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "\n",
    "        print(\"\\nQ-LIME Pi Explanation:\")\n",
    "        for word, weight in contributions_qlime_sorted:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "        \n",
    "        #print(\"\\n weights\", clf.coef_[0])\n",
    "        qlime_time = time.time() - start_qlime\n",
    "        qlime_times.append(qlime_time)\n",
    "\n",
    "        # top 5 (by absolute value)\n",
    "        nonzero_indices = [\n",
    "            (i, abs(contributions_qlime[i])) \n",
    "            for i in range(len(contributions_qlime))\n",
    "        ]\n",
    "        top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "        top_words_qlime = [\n",
    "            vectorizer.get_feature_names_out()[i2]\n",
    "            for (i2, val) in top_indices_qlime\n",
    "        ]\n",
    "\n",
    "        # measure overlap\n",
    "        overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "        overlaps.append(len(overlap))\n",
    "\n",
    "    # Summary\n",
    "    results = {\n",
    "        \"local_accuracy\": np.mean(instance_local_accuracies),\n",
    "        \"lime_time_avg\": round(np.mean(lime_times), 4),\n",
    "        \"qlime_time_avg\": round(np.mean(qlime_times), 4),\n",
    "        \"overlap_avg\": round(np.mean(overlaps), 4),\n",
    "        \"global_acc\": np.mean(test_acc)\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=5000, max_features=15, stopwords=True, lime_num_samples=300, shots=100,stop_words=english,n_test_explanations=5\n",
      "Loading cached logistic from cached_classifier_ns5000_mf15_swTrue_logistic_classifier_seed42.pkl\n",
      "Global accuracy: 0.657\n",
      "text sample in iran, women are not permitted to attend men's sporting events, apparently to \"protect\" them from all the cursing and foul language they might hear emanating from the male fans (so since men can't restrain or behave themselves, women are forced to suffer. go figure.). \"offside\" tells the tale of a half dozen or so young women who, dressed like men, attempt to sneak into the high-stakes match between iran and bahrain that, in 2005, qualified iran to go to the world cup (the movie was actually filmed in large part during that game).\"offside\" is a slice-of-life comedy that will remind you of all those great humanistic films (\"the shop on main street,\" \"loves of a blonde,\" \"closely watched trains\" etc.) that flowed out of communist czechoslovakia as part of the \"prague miracle\" in the mid 1960's. as with many of those works, \"offside\" is more concerned with observing life than with devising any kind of elaborately contrived fictional narrative. indeed, it is the simplicity of the setup and the naturalism of the style that make the movie so effective.once their ruse is discovered, the girls are corralled into a small pen right outside the stadium where they can hear the raucous cheering emanating from the game inside. stuck where they are, all they can do is plead with the security guards to let them go in, guards who are basically bumbling, good-natured lads who are compelled to do their duty as a part of their compulsory military service. even most of the men going into the stadium don't seem particularly perturbed at the thought of these women being allowed in. still the prohibition persists. yet, how can one not be impressed by the very real courage and spunk displayed by these women as they go up against a system that continues to enforce such a ridiculously regressive and archaic restriction? and, yet, the purpose of these women is not to rally behind a cause or to make a \"point.\" they are simply obsessed fans with a burning desire to watch a soccer game and, like all the men in the country, cheer on their team.it's hard to tell just how much of the dialogue is scripted and how much of it is extemporaneous, but, in either case, the actors, with their marvelously expressive faces, do a magnificent job making each moment seem utterly real and convincing. mohammad kheir-abadi and shayesteh irani are notable standouts in a uniformly excellent cast. the structure of the film is also very loose and freeform, as writer/director jafar panahi and co-writer shadmehr rastin focus for a few brief moments on one or two of the characters, then move smoothly and effortlessly onto others. with this documentary-type approach, we come to feel as if we are witnessing an actual event unfolding in \"real time.\" very often, it's quite easy for us to forget we're actually watching a movie.it was a very smart move on the part of the filmmakers to include so much good-natured humor in the film (it's what the czech filmmakers did as well), the better to point up the utter absurdity of the situation and broaden the appeal of the film for audiences both domestic and foreign. \"offside\" is obviously a cry for justice, but it is one that is made all the more effective by its refusal to make of its story a heavy-breathing tragedy. instead, it realizes that nothing breaks down social barriers quite as efficiently as humor and an appeal to the audience's common humanity. and isn't that what true art is supposed to be all about? in its own quiet, understated way, \"offside\" is one of the great, under-appreciated gems of 2007. bin_features [0 1 1 1 1 1 1 1 1 0 0 1 1 1 1]\n",
      "Classical LIME Explanation:\n",
      "Word: great, Importance: 0.24120139209888247\n",
      "Word: don, Importance: 0.07916210744316085\n",
      "Word: story, Importance: 0.07430713368128992\n",
      "Word: make, Importance: 0.069664880183138\n",
      "Word: movie, Importance: 0.06455172404471507\n",
      "Word: just, Importance: 0.06155546332053802\n",
      "Word: way, Importance: 0.0336905512071765\n",
      "Word: good, Importance: 0.0315330027856027\n",
      "Word: any, Importance: 0.006962396603290193\n",
      "Word: humor, Importance: 0.006131456811263356\n",
      "Word: realizes, Importance: 0.0037656646619569764\n",
      "Word: small, Importance: 0.002062873389885945\n",
      "Word: humanistic, Importance: 0.001970693637815946\n",
      "Word: more, Importance: 0.001009913088367491\n",
      "Word: real, Importance: 0.00018949044003866347\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.44955005188327973\n",
      "Word: way, Importance: 0.3689520924883828\n",
      "Word: story, Importance: 0.33864023198471643\n",
      "Word: time, Importance: 0.1924328924747754\n",
      "Word: people, Importance: 0.1623825218034235\n",
      "Word: make, Importance: 0.1504883695865371\n",
      "Word: movie, Importance: 0.10656911849933348\n",
      "Word: good, Importance: 0.08571183202143284\n",
      "Word: like, Importance: 0.068930985001041\n",
      "Word: really, Importance: 0.06281562379516337\n",
      "Word: film, Importance: 0.05319067016646484\n",
      "Word: just, Importance: 0.033277310645348446\n",
      "Word: great, Importance: 0.019013443728151058\n",
      "Word: don, Importance: 0.016643230986969293\n",
      "Word: watch, Importance: 0.01603093486244145\n",
      "text sample joseph l. mankiewicz's sleuth didn't need a remake. it's a thoroughly well made film that stands up well to this day. however, given that the modern day remake machine is currently in full swing; i really can't say i'm surprised to see the film updated for modern audience. the plot remains identical to the original film and at its core we have the story of a young man, milo tindle, who goes off to see an older man, andrew wyke, to discuss a divorce as the younger man is having an affair with the older man's wife. from there, a game of cat and mouse ensues. its clear right from the outset that director kenneth branagh wanted to add a different touch to this film and he does so by way of the central location, which has been changed from the charming games-ridden country house of the original to a technical marvel kitted out with layers of security equipment. i'm glad that the director chose to make this change as nobody wants to see a remake that directly copies of the original; plus there's the fact that the location is well used and always nice to look at. unfortunately, however, the positive elements of sleuth 2007 end there.the original film was over two hours long, while this remake is only just a shade over eighty minutes. naturally, therefore, that means that this version has less about it; and unfortunately it's the characters that suffer. the plot is also rushed and we get into the first twist in the tale far too quickly and before we are given any chance to actually understand why and how these events can be taking place. the film does not build the characters, or the rapport between them, enough to make sure that their relationship makes sense. one major thing that has been changed about the older character is his obsession; in the original he was obsessed with games which turned out to be very important once the twists come into play. here he has some kind of security fetish that doesn't really mean anything by the end. kenneth branagh's handling of the film allows for a classy score but the class ends there. the original thrived on it, but this film is happy merely to soil itself with expletives on numerous, and mostly unwarranted, occasions; which cheapen the whole thing. the final twist in the tale is completely different to how it was in the original and ensures that the film boils down to a really hideous conclusion. after spending two hours with the original i understood, respected and liked both characters presented in the film - after eighty minutes of this, i hated them both. i do have some respect for branagh for not merely rolling out a carbon copy of the original film; but this is not a good adaptation of the great anthony shaffer play. bin_features [0 0 1 1 1 1 0 1 0 0 1 1 0 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: great, Importance: 0.22180591094599394\n",
      "Word: story, Importance: 0.0689073773470438\n",
      "Word: make, Importance: 0.05883561763835819\n",
      "Word: just, Importance: 0.0541953834679404\n",
      "Word: way, Importance: 0.02794579012699329\n",
      "Word: good, Importance: 0.026600834253600627\n",
      "Word: first, Importance: 0.00445740620334915\n",
      "Word: have, Importance: 0.003987662943216485\n",
      "Word: positive, Importance: 0.00235566035752962\n",
      "Word: having, Importance: 0.0019320068522402953\n",
      "Word: charming, Importance: 0.001895554654170991\n",
      "Word: does, Importance: 0.001856032129348645\n",
      "Word: respect, Importance: 0.0017551074467343226\n",
      "Word: class, Importance: 0.0011226378828991521\n",
      "Word: respected, Importance: 0.0008370798272819334\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.6096242420291018\n",
      "Word: story, Importance: 0.3689745638883098\n",
      "Word: don, Importance: 0.32950868778899123\n",
      "Word: film, Importance: 0.29937294414660875\n",
      "Word: like, Importance: 0.2965593249175381\n",
      "Word: movie, Importance: 0.256820929253859\n",
      "Word: make, Importance: 0.23881143232758628\n",
      "Word: way, Importance: 0.23041420409966085\n",
      "Word: really, Importance: 0.19998699484968818\n",
      "Word: watch, Importance: 0.18791780117663204\n",
      "Word: people, Importance: 0.14320195660025659\n",
      "Word: just, Importance: 0.11021921497370712\n",
      "Word: good, Importance: 0.027231323706989996\n",
      "Word: time, Importance: 0.024237310939430268\n",
      "Word: great, Importance: 0.0204260342893281\n",
      "text sample brothers with psychokinetic powers (yes, really) duel not just for debra winger's affections but really over a secret from their childhood that left them at odds over their powers.there are surreal touches (the fire brigade that act like a singing greek chorus), but there is also humour and romance. the soundtrack is great similar to the way american werewolf in london used every great wolf song they could get ~ but with fire ~ and i don't think i'll ever forget dennis quaid (mmmmm dennis quaid), setting his own trailer a rockin' to 'she's a lady' ~ priceless ;)best line missing from the quotes section btw ~ 'once you've had a clown, you never go back!'i love this movie (i ordered the dvd from the us) and if the comments written by the kind of people who'd be happier with legally blond 3 don't put you off ~ give it a try :) bin_features [0 1 0 0 1 1 1 0 1 1 1 0 0 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: great, Importance: 0.2513340238076209\n",
      "Word: don, Importance: 0.08811547551386173\n",
      "Word: movie, Importance: 0.06806757808392665\n",
      "Word: just, Importance: 0.06498003187008108\n",
      "Word: way, Importance: 0.0323022541528885\n",
      "Word: really, Importance: 0.008498238503737201\n",
      "Word: brothers, Importance: 0.0036658103318644047\n",
      "Word: by, Importance: 0.0027329752837002667\n",
      "Word: with, Importance: 0.002448342335743426\n",
      "Word: his, Importance: 0.002069282117082099\n",
      "Word: off, Importance: 0.0020318720158202315\n",
      "Word: it, Importance: 0.0019077876108125366\n",
      "Word: written, Importance: 0.0017607561598968605\n",
      "Word: she, Importance: 0.0008253720594785127\n",
      "Word: romance, Importance: 8.251081905516277e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.33736702238335814\n",
      "Word: just, Importance: 0.18399232062440923\n",
      "Word: way, Importance: 0.18399232062440923\n",
      "Word: film, Importance: 0.18237756766405855\n",
      "Word: watch, Importance: 0.16121765244329656\n",
      "Word: time, Importance: 0.14361976225398665\n",
      "Word: really, Importance: 0.1342534996810102\n",
      "Word: great, Importance: 0.13083103435325888\n",
      "Word: story, Importance: 0.1032825748019699\n",
      "Word: movie, Importance: 0.09470650501233335\n",
      "Word: like, Importance: 0.07190926257863317\n",
      "Word: good, Importance: 0.06878633787343913\n",
      "Word: people, Importance: 0.05325339763850223\n",
      "Word: make, Importance: 0.04309685077958669\n",
      "Word: don, Importance: 0.0\n",
      "text sample the bourne ultimatum (2007) review: after a thrilling set of two, we get the final installment. here's my take:the bourne ultimatum has it all. we have jason bourne(matt damon) on the coattails of the ones who know everything. he has been running for too long. this time, it ends.the bourne ultimatum has a great plot, awesome writing, fantastic direction, suspense, and some of the best action of the summer. matt damon delivers possibly his best performance to date. he has the conviction and swelling desire of the troubled assassin.there are some intelligent humor here and some fine suspense. the reactions to certain events will have you either laughing(in a good way) or cheering on. (or both) i heard a lot of intelligent laughter in the theater and lots of clapping. the audience was loving it.the bourne ultimatum delivers all in a nicely gift-wrapped package. all of the goods and then some. this is, in my opinion the best movie this summer.the last word: excellent conclusion. the best of the trilogy. this is how summer movie thrillers should be done. i love the bourne trilogy. bin_features [0 0 0 1 1 0 0 0 1 0 0 0 1 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: great, Importance: 0.22573148144099667\n",
      "Word: movie, Importance: 0.05857495486704038\n",
      "Word: way, Importance: 0.02950456633303596\n",
      "Word: good, Importance: 0.024902147897580846\n",
      "Word: time, Importance: 0.008352070092085953\n",
      "Word: laughing, Importance: 0.004159448652139717\n",
      "Word: conclusion, Importance: 0.0038300481912153096\n",
      "Word: bourne, Importance: 0.0024538011260794575\n",
      "Word: either, Importance: 0.0021061573183098125\n",
      "Word: action, Importance: 0.0019071496804341317\n",
      "Word: direction, Importance: 0.00166487846957204\n",
      "Word: we, Importance: 0.0015809175834779216\n",
      "Word: trilogy, Importance: 0.0009707105623306019\n",
      "Word: two, Importance: 0.0007534500076407184\n",
      "Word: should, Importance: 0.0005562824776023672\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.5020741449357757\n",
      "Word: really, Importance: 0.3078273270136991\n",
      "Word: people, Importance: 0.28155806929458327\n",
      "Word: like, Importance: 0.22736642374773097\n",
      "Word: film, Importance: 0.21273146229409823\n",
      "Word: way, Importance: 0.20280368360222967\n",
      "Word: great, Importance: 0.19401391469758056\n",
      "Word: movie, Importance: 0.19051862947231135\n",
      "Word: watch, Importance: 0.17840733152084998\n",
      "Word: just, Importance: 0.0935301987141357\n",
      "Word: time, Importance: 0.06110942726450663\n",
      "Word: good, Importance: 0.05123715136978524\n",
      "Word: story, Importance: 0.049562406758421385\n",
      "Word: don, Importance: 0.0244913026982555\n",
      "Word: make, Importance: 0.011601905432885307\n",
      "text sample i am commenting on this miniseries from the perspective of someone who read the novel first. and from that perspective i can honestly say that while enjoyable, i can see why it hasn't been rebroadcast anytime recently. more specifically, this mini has some serious problems, such as:1) it is terribly miscast. the actors who played the younger generation were all 15 to 20 years older than the characters. ali mcgraw (45 at the time) was playing natalie jastrow who was supposed to be about 26. jan-michael vincent (39 at the time) was playing byron henry who was supposed to be about 22. the other henry children, and pamela tudsbury, were also played by actors way too old for characters who were supposed to be in their 20's.2) some of the acting was absolutely awful. ali mcgraw at times almost made this mini unwatchable. i have seen more convincing performances in high school plays. 3) the directing was poor. to be fair to ali mcgraw, the bad acting and character development were probably the directing. the portrayal of hitler was way overdone. his character came off looking and behaving more like a cartoon villain than the charismatic, sometimes charming, but always diabolical genius herman wouk painted him as in the novel. some of the other characters are done so stereotypically (berel jastrow) they do not gain the depth of character that wouk created for them.4) this mini is very dated. the hokey music, the pretentious narration (it sounded like a junior high school history film narration), and the entire prime-time soap opera feel of the mini made it almost comical at times. also, too often byron and natalie are costumed and made up to look like they are in 1979 rather than 1939.someone who watches this without the benefit of reading the novel first will probably not sit through it all, because it will come off more as a late 70's / early 80's \"take myself too seriously\" prime-time soap drama, rather than the television version of what is certainly a modern american classic.remakes of older movies and the like are sometimes poorly done, but this is probably one case where a creative and inspired director could make a very stunning, memorable, and critically acclaimed production. i don't ever see that happening since a remake would have to be just as long (15 hours) or longer to do it right, and given the short attention span of most of the current american viewing public, it wouldn't fly. bin_features [1 1 1 0 0 1 1 1 0 0 0 0 1 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: bad, Importance: 0.2481523698974412\n",
      "Word: don, Importance: 0.07211180026450952\n",
      "Word: make, Importance: 0.057067675465790856\n",
      "Word: just, Importance: 0.05143867273804168\n",
      "Word: way, Importance: 0.025359321561650373\n",
      "Word: film, Importance: 0.011909824604405816\n",
      "Word: time, Importance: 0.008829724053415953\n",
      "Word: inspired, Importance: 0.0053965464606102785\n",
      "Word: since, Importance: 0.0048001298705145365\n",
      "Word: mcgraw, Importance: 0.004204772255837195\n",
      "Word: been, Importance: 0.003991047319045131\n",
      "Word: villain, Importance: 0.0015401845150933848\n",
      "Word: director, Importance: 0.0015024579692277781\n",
      "Word: what, Importance: 0.0011589498913188818\n",
      "Word: often, Importance: 0.0008705986019035749\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.622587493097569\n",
      "Word: make, Importance: 0.391469400636037\n",
      "Word: story, Importance: 0.3649344849723865\n",
      "Word: time, Importance: 0.35997124489410537\n",
      "Word: don, Importance: 0.2566893057572543\n",
      "Word: movie, Importance: 0.17850533731358315\n",
      "Word: really, Importance: 0.09515642571585231\n",
      "Word: like, Importance: 0.08013210697289475\n",
      "Word: good, Importance: 0.05867885761915553\n",
      "Word: just, Importance: 0.05464921392370367\n",
      "Word: watch, Importance: 0.05462101709323344\n",
      "Word: film, Importance: 0.050846435105913385\n",
      "Word: way, Importance: 0.050846435105913385\n",
      "Word: bad, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "text sample i can't say i'm all that experienced in misty mundae flicks having seen only a handful, but it's obvious that this was made on a shoestring, and while it might have been respectable that the filmmakers were able to make a tomb raider rip-off inside a garage, it isn't because it's completely obvious that this is what they were doing. the film only runs for forty five minutes, and this is definitely a good thing as there isn't nearly enough plot here to stretch it out for any longer. it has something to do with an evil nazi scientist (who looks about as evil as a porn star playing a nazi scientist ever could), a mummy, which is clearly a man wrapped up in toilet roll and misty - this film's version of tomb raider, who keeps her top on for much less time than angelina jolie did in the big budget version. i have to say that even in spite of its shortcomings, this film could have been better. it's got misty mundae for a start, and even better than that if you ask me is the fact that it also stars the even hotter darian caine. the pair gets to engage in all the lesbian sex that you would expect from a seduction cinema film and this is at the expense of the nonexistent plot, although that isn't really a bad thing. obviously, this is a rubbish film - but the fact that it's short is to its credit, and if you're after a bit of lesbian sex, you could do worse. bin_features [1 0 1 1 0 0 0 1 0 0 1 0 1 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: bad, Importance: 0.2864666825205997\n",
      "Word: make, Importance: 0.06945683684613399\n",
      "Word: good, Importance: 0.025170615818099207\n",
      "Word: time, Importance: 0.010725667277913372\n",
      "Word: film, Importance: 0.010499912048371704\n",
      "Word: really, Importance: 0.008563320835764374\n",
      "Word: this, Importance: 0.0028456257791416614\n",
      "Word: have, Importance: 0.0018787140626007007\n",
      "Word: raider, Importance: 0.0014714408301808699\n",
      "Word: handful, Importance: 0.0012302093602105197\n",
      "Word: it, Importance: 0.0010942516897771114\n",
      "Word: expense, Importance: 0.0006380126089580256\n",
      "Word: playing, Importance: 0.0004551806385475311\n",
      "Word: flicks, Importance: 0.0004537085996757881\n",
      "Word: rip, Importance: 0.00012584746480298975\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.5774634125654359\n",
      "Word: time, Importance: 0.3814830489812334\n",
      "Word: bad, Importance: 0.3694417159133716\n",
      "Word: good, Importance: 0.3488111477921214\n",
      "Word: like, Importance: 0.3217788023217356\n",
      "Word: really, Importance: 0.3217788023217356\n",
      "Word: make, Importance: 0.3102467256393142\n",
      "Word: movie, Importance: 0.1715222040028039\n",
      "Word: story, Importance: 0.10493345019750386\n",
      "Word: don, Importance: 0.09954656758965993\n",
      "Word: way, Importance: 0.04723767648141944\n",
      "Word: film, Importance: 0.042890075880189116\n",
      "Word: watch, Importance: 0.0422296796255964\n",
      "Word: just, Importance: 0.0046306808216640405\n",
      "Word: people, Importance: 0.0\n",
      "text sample eddie murphy is one of the funniest comedians ever - probably the funniest. delirious is the best stand-up comedy i've ever seen and it is a must-have for anyone who loves a good laugh!! i've watched this movie hundreds of times and every time i see it - i still have side-splitting fun. this is definitely one for your video library. i guarantee that you will have to watch it several times in order to hear all the jokes because you will be laughing so much - that you will miss half of them! delirious is hilarious!although there are a lot of funny comedians out there - after watching this stand-up comedy, most of them will seem like second-class citizens. if you have never seen it - get it, watch it - and you will love it!! it will make you holler!!! :-) bin_features [0 0 0 1 0 0 1 1 1 0 0 0 1 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: make, Importance: 0.08005357947497001\n",
      "Word: movie, Importance: 0.07604601860125429\n",
      "Word: good, Importance: 0.03228564775430748\n",
      "Word: watch, Importance: 0.011962913039533627\n",
      "Word: time, Importance: 0.011352577295567528\n",
      "Word: like, Importance: 0.0009432340567381827\n",
      "Word: second, Importance: 0.0009416916058458515\n",
      "Word: seen, Importance: 0.0006479241540281135\n",
      "Word: funny, Importance: 0.0006379479038981598\n",
      "Word: best, Importance: 0.0003327968064370398\n",
      "Word: there, Importance: 0.00024188380163655678\n",
      "Word: one, Importance: 0.00019477430724410436\n",
      "Word: murphy, Importance: 5.731414564375562e-05\n",
      "Word: never, Importance: 5.478368086109617e-05\n",
      "Word: definitely, Importance: 2.65167749686318e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.31474053521292505\n",
      "Word: bad, Importance: 0.21637782830940083\n",
      "Word: story, Importance: 0.20140593036594573\n",
      "Word: really, Importance: 0.1731523866321873\n",
      "Word: film, Importance: 0.16422725491280826\n",
      "Word: time, Importance: 0.1376342440660499\n",
      "Word: just, Importance: 0.07335279540485556\n",
      "Word: watch, Importance: 0.07152508106285521\n",
      "Word: movie, Importance: 0.06690151608362799\n",
      "Word: make, Importance: 0.04603213028211006\n",
      "Word: way, Importance: 0.03781735635485284\n",
      "Word: don, Importance: 0.0307568155720212\n",
      "Word: people, Importance: 0.02510395219853656\n",
      "Word: like, Importance: 0.024455494980296533\n",
      "Word: good, Importance: 0.019838606674110137\n",
      "text sample if i could say it was better than gymkata, i at least felt my money was not totally wasted.then i saw steven segal's on deadly ground.this movie should see a resurrection though on mst 3k. if santa claus conquers the martians could make tom servo's head explode, one wonders what mayhem this movie could cause.there is a very good reason why kurt thomas never had a movie career.the writers of this dreck should be forced to wear placards every day of their lives that say \"bitch slap me! i was a writer on gymkata.\" bin_features [0 0 0 1 0 0 0 1 1 0 0 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: make, Importance: 0.08071338843361096\n",
      "Word: movie, Importance: 0.07636964595005315\n",
      "Word: good, Importance: 0.032227700075454356\n",
      "Word: see, Importance: 0.0005135136611158791\n",
      "Word: tom, Importance: 0.0004911518454194224\n",
      "Word: writer, Importance: 0.0004291842873903831\n",
      "Word: it, Importance: 0.00037490079843075224\n",
      "Word: not, Importance: 0.00028449366636864383\n",
      "Word: one, Importance: 0.0002762951521489276\n",
      "Word: could, Importance: 0.00023615293017402447\n",
      "Word: thomas, Importance: 0.00014646853805096622\n",
      "Word: deadly, Importance: 9.979998835625564e-05\n",
      "Word: their, Importance: 9.074157732131268e-05\n",
      "Word: i, Importance: 9.036894231016317e-05\n",
      "Word: there, Importance: 6.715425311030384e-06\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.23700263203428723\n",
      "Word: story, Importance: 0.21486901587755952\n",
      "Word: way, Importance: 0.1989815664154561\n",
      "Word: bad, Importance: 0.1731734072214441\n",
      "Word: film, Importance: 0.09881744406354237\n",
      "Word: really, Importance: 0.09093485292013587\n",
      "Word: just, Importance: 0.07391347010976024\n",
      "Word: watch, Importance: 0.06226898868524505\n",
      "Word: movie, Importance: 0.049559447178833094\n",
      "Word: like, Importance: 0.0449425588726467\n",
      "Word: make, Importance: 0.0449425588726467\n",
      "Word: don, Importance: 0.018044652412564255\n",
      "Word: time, Importance: 0.012535110306642061\n",
      "Word: good, Importance: 0.0\n",
      "Word: people, Importance: 0.0\n",
      "text sample what a load of rubbish.. i can't even begin to describe how awful this film was. the rating it has here is really hard to believe.avoid... particularly if you enjoyed the first ginger snaps. the first one was well written, well directed, well executed.. a brilliant film with a fantastic aesthetic and atmosphere. the second one was 'alrite'- decent as a self-standing film, but clearly not up to the level of the first... the third is an insult to the series, period. i rate the films: 10, 6, 1. it's that bad.oh, and yes it really is set in the past, the sisters are still called ginger and b fitzgerald... all muddled in with some half-assed native american mythology. the sisters don't have any real story, or progression, or even a clear relationship... they're just trying to survive and be 'together forever'. that's about as deep as it gets.staggered that the girls agreed to be in this pile-of-shite, after reading the script.oh and another thing, staging of action was terrible- people appearing from nowhere regularly, like the girls turn around and there's an elaborate candle-lit setup with a mystic native american woman just sitting there, about to go into a speech. sets were terrible, couldn't get away from the fact that it was all obviously based in a set, which really didn't help. also, there was consistently snow outside the camp, but not a trace inside (..on the set).arrghh,,, so bad! i really was hoping it would be at least as good as the second one. bin_features [1 1 1 1 0 1 1 0 0 1 1 1 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: bad, Importance: 0.27626046434860557\n",
      "Word: don, Importance: 0.08238590260469526\n",
      "Word: story, Importance: 0.06788773348631928\n",
      "Word: just, Importance: 0.06114040555878344\n",
      "Word: good, Importance: 0.02158183662868643\n",
      "Word: film, Importance: 0.01267718855777347\n",
      "Word: really, Importance: 0.010449595157916129\n",
      "Word: that, Importance: 0.00697228470618819\n",
      "Word: alrite, Importance: 0.006039568669711473\n",
      "Word: as, Importance: 0.0048410924799403895\n",
      "Word: people, Importance: 0.00481639497825891\n",
      "Word: 10, Importance: 0.0028082217894540677\n",
      "Word: go, Importance: 0.0016014818101879051\n",
      "Word: you, Importance: 0.0012256410049361342\n",
      "Word: well, Importance: 0.0004598550267843338\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.5852085112633216\n",
      "Word: watch, Importance: 0.3958431725035977\n",
      "Word: just, Importance: 0.35642632191436896\n",
      "Word: time, Importance: 0.34841963528173736\n",
      "Word: bad, Importance: 0.2961744755103006\n",
      "Word: good, Importance: 0.2929173305802174\n",
      "Word: story, Importance: 0.2929173305802174\n",
      "Word: make, Importance: 0.2747679271252169\n",
      "Word: like, Importance: 0.26186847435116234\n",
      "Word: movie, Importance: 0.19999705059125897\n",
      "Word: way, Importance: 0.114210220972886\n",
      "Word: film, Importance: 0.07188236850579571\n",
      "Word: don, Importance: 0.043003136514448065\n",
      "Word: people, Importance: 0.02586120326139013\n",
      "Word: really, Importance: 0.010426672422020056\n",
      "text sample ummm, please forgive me, but weren't more than half the characters missing? in the original novel, valjean is a man imprisoned for 19 years for stealing a loaf of bread and then attempting several times to escape. he breaks parole and is pursued relentlessly by the police inspector javert. along the way there are many characters that weren't in this version. some worth mentioning would be fantine, cosette, m & mme. thenardier, eponine, marius, gavroche, and enjolras. the only character with the same name is javert. i was confused and frustrated throughout the whole movie, trying to see how it was in any way connected to victor hugo's epic novel. bin_features [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: movie, Importance: 0.07580941599070783\n",
      "Word: way, Importance: 0.03581653855218195\n",
      "Word: to, Importance: 0.0006171485243142167\n",
      "Word: novel, Importance: 0.0005328284341130389\n",
      "Word: years, Importance: 0.0005287240369452082\n",
      "Word: fantine, Importance: 0.000511493521369788\n",
      "Word: pursued, Importance: 0.00043090774590187684\n",
      "Word: was, Importance: 0.00038704389432358833\n",
      "Word: would, Importance: 0.00038190925130710527\n",
      "Word: imprisoned, Importance: 0.000270959835673644\n",
      "Word: trying, Importance: 0.00026043548628240773\n",
      "Word: original, Importance: 0.00015945683863230278\n",
      "Word: and, Importance: 0.0001455030989008022\n",
      "Word: marius, Importance: 9.92024398115796e-05\n",
      "Word: gavroche, Importance: 7.352008770889291e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.31604087811849435\n",
      "Word: great, Importance: 0.2780855365851995\n",
      "Word: story, Importance: 0.12735544125912301\n",
      "Word: watch, Importance: 0.09099496374780403\n",
      "Word: like, Importance: 0.0787543856923536\n",
      "Word: movie, Importance: 0.0787543856923536\n",
      "Word: don, Importance: 0.06102287774174031\n",
      "Word: film, Importance: 0.05634397691636839\n",
      "Word: people, Importance: 0.0416067323153404\n",
      "Word: just, Importance: 0.036126830539558674\n",
      "Word: time, Importance: 0.012754829756679187\n",
      "Word: really, Importance: 0.011685701623930678\n",
      "Word: make, Importance: 0.004628680863175028\n",
      "Word: good, Importance: 0.003645870178747068\n",
      "Word: way, Importance: 0.0\n",
      "Results => {'num_samples': 5000, 'max_features': 15, 'stopwords': True, 'lime_num_samples': 300, 'shots': 100, 'local_accuracy': np.float64(0.7), 'lime_time_avg': np.float64(0.0596), 'qlime_time_avg': np.float64(0.108), 'global_acc': np.float64(0.657)}\n",
      "\n",
      "All done! Saved results to 'results_expanded_flips.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # Parameter grid to systematically vary certain settings\n",
    "    param_grid = {\n",
    "        \"num_samples\": [5000],\n",
    "        \"max_features\": [15],\n",
    "        \"stopwords_option\": [True],\n",
    "        \"lime_num_samples\": [300],\n",
    "        # Shots: None => analytic mode, 100 => finite sampling\n",
    "        \"shots\": [100],\n",
    "        \"stop_words\": ['english'],\n",
    "        \"n_test_explanations\": [5]\n",
    "    }\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    all_results = []\n",
    "\n",
    "    for combo in combos:\n",
    "        (num_samples_, max_features_, stopwords_, lime_samps_, shots_, stop_words_, n_test_explanations_) = combo\n",
    "        \n",
    "        print(\"\\n==================================\")\n",
    "        print(f\"Running experiment with: \"\n",
    "              f\"num_samples={num_samples_}, \"\n",
    "              f\"max_features={max_features_}, \"\n",
    "              f\"stopwords={stopwords_}, \"\n",
    "              f\"lime_num_samples={lime_samps_}, \"\n",
    "              f\"shots={shots_},\"\n",
    "              f\"stop_words={stop_words_},\"\n",
    "              f\"n_test_explanations={n_test_explanations_}\")\n",
    "        \n",
    "        res = run_experiment(\n",
    "            num_samples=num_samples_,\n",
    "            max_features=max_features_,\n",
    "            stopwords_option=stopwords_,\n",
    "            lime_num_samples=lime_samps_,\n",
    "            shots=shots_,\n",
    "            stop_words=stop_words_,\n",
    "            n_test_explanations=n_test_explanations_,\n",
    "            \n",
    "            \n",
    "        )\n",
    "        res_row = {\n",
    "            \"num_samples\": num_samples_,\n",
    "            \"max_features\": max_features_,\n",
    "            \"stopwords\": stopwords_,\n",
    "            \"lime_num_samples\": lime_samps_,\n",
    "            \"shots\": shots_,\n",
    "            \"local_accuracy\": res[\"local_accuracy\"],\n",
    "            \"lime_time_avg\": res[\"lime_time_avg\"],\n",
    "            \"qlime_time_avg\": res[\"qlime_time_avg\"],\n",
    "            # \"overlap_avg\": res[\"overlap_avg\"],\n",
    "            # \"n_test_explanations\": n_test_explanations_,\n",
    "            # \"stop_words\": stop_words_,\n",
    "            \"global_acc\": res[\"global_acc\"],\n",
    "\n",
    "        }\n",
    "        print(\"Results =>\", res_row)\n",
    "        all_results.append(res_row)\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"results_expanded_flips.csv\", index=False)\n",
    "    print(\"\\nAll done! Saved results to 'results_expanded_flips.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
